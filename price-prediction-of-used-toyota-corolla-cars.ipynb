{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7360189,"sourceType":"datasetVersion","datasetId":4275173}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-10T05:01:33.708487Z","iopub.execute_input":"2024-01-10T05:01:33.709383Z","iopub.status.idle":"2024-01-10T05:01:33.719562Z","shell.execute_reply.started":"2024-01-10T05:01:33.709324Z","shell.execute_reply":"2024-01-10T05:01:33.718282Z"},"trusted":true},"execution_count":163,"outputs":[{"name":"stdout","text":"/kaggle/input/price-of-used-toyota-corolla-cars/ToyotaCorolla.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/price-of-used-toyota-corolla-cars/ToyotaCorolla.csv\")\ndf","metadata":{"execution":{"iopub.status.busy":"2024-01-10T05:01:33.753973Z","iopub.execute_input":"2024-01-10T05:01:33.754445Z","iopub.status.idle":"2024-01-10T05:01:33.801757Z","shell.execute_reply.started":"2024-01-10T05:01:33.754407Z","shell.execute_reply":"2024-01-10T05:01:33.800629Z"},"trusted":true},"execution_count":164,"outputs":[{"execution_count":164,"output_type":"execute_result","data":{"text/plain":"        Id                                              Model  Price  \\\n0        1      TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors  13500   \n1        2      TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors  13750   \n2        3      TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors  13950   \n3        4      TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors  14950   \n4        5        TOYOTA Corolla 2.0 D4D HATCHB SOL 2/3-Doors  13750   \n...    ...                                                ...    ...   \n1431  1438         TOYOTA Corolla 1.3 16V HATCHB G6 2/3-Doors   7500   \n1432  1439  TOYOTA Corolla 1.3 16V HATCHB LINEA TERRA 2/3-...  10845   \n1433  1440  TOYOTA Corolla 1.3 16V HATCHB LINEA TERRA 2/3-...   8500   \n1434  1441  TOYOTA Corolla 1.3 16V HATCHB LINEA TERRA 2/3-...   7250   \n1435  1442        TOYOTA Corolla 1.6 LB LINEA TERRA 4/5-Doors   6950   \n\n      Age_08_04  Mfg_Month  Mfg_Year     KM Fuel_Type   HP  Met_Color  ...  \\\n0            23         10      2002  46986    Diesel   90          1  ...   \n1            23         10      2002  72937    Diesel   90          1  ...   \n2            24          9      2002  41711    Diesel   90          1  ...   \n3            26          7      2002  48000    Diesel   90          0  ...   \n4            30          3      2002  38500    Diesel   90          0  ...   \n...         ...        ...       ...    ...       ...  ...        ...  ...   \n1431         69         12      1998  20544    Petrol   86          1  ...   \n1432         72          9      1998  19000    Petrol   86          0  ...   \n1433         71         10      1998  17016    Petrol   86          0  ...   \n1434         70         11      1998  16916    Petrol   86          1  ...   \n1435         76          5      1998      1    Petrol  110          0  ...   \n\n     Powered_Windows  Power_Steering  Radio  Mistlamps  Sport_Model  \\\n0                  1               1      0          0            0   \n1                  0               1      0          0            0   \n2                  0               1      0          0            0   \n3                  0               1      0          0            0   \n4                  1               1      0          1            0   \n...              ...             ...    ...        ...          ...   \n1431               1               1      0          1            1   \n1432               0               1      0          0            1   \n1433               0               1      0          0            0   \n1434               0               0      0          0            0   \n1435               0               1      0          0            0   \n\n      Backseat_Divider  Metallic_Rim  Radio_cassette  Parking_Assistant  \\\n0                    1             0               0                  0   \n1                    1             0               0                  0   \n2                    1             0               0                  0   \n3                    1             0               0                  0   \n4                    1             0               0                  0   \n...                ...           ...             ...                ...   \n1431                 1             0               0                  0   \n1432                 1             0               0                  0   \n1433                 1             0               0                  0   \n1434                 1             0               0                  0   \n1435                 0             0               0                  0   \n\n      Tow_Bar  \n0           0  \n1           0  \n2           0  \n3           0  \n4           0  \n...       ...  \n1431        0  \n1432        0  \n1433        0  \n1434        0  \n1435        0  \n\n[1436 rows x 39 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Model</th>\n      <th>Price</th>\n      <th>Age_08_04</th>\n      <th>Mfg_Month</th>\n      <th>Mfg_Year</th>\n      <th>KM</th>\n      <th>Fuel_Type</th>\n      <th>HP</th>\n      <th>Met_Color</th>\n      <th>...</th>\n      <th>Powered_Windows</th>\n      <th>Power_Steering</th>\n      <th>Radio</th>\n      <th>Mistlamps</th>\n      <th>Sport_Model</th>\n      <th>Backseat_Divider</th>\n      <th>Metallic_Rim</th>\n      <th>Radio_cassette</th>\n      <th>Parking_Assistant</th>\n      <th>Tow_Bar</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors</td>\n      <td>13500</td>\n      <td>23</td>\n      <td>10</td>\n      <td>2002</td>\n      <td>46986</td>\n      <td>Diesel</td>\n      <td>90</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors</td>\n      <td>13750</td>\n      <td>23</td>\n      <td>10</td>\n      <td>2002</td>\n      <td>72937</td>\n      <td>Diesel</td>\n      <td>90</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors</td>\n      <td>13950</td>\n      <td>24</td>\n      <td>9</td>\n      <td>2002</td>\n      <td>41711</td>\n      <td>Diesel</td>\n      <td>90</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors</td>\n      <td>14950</td>\n      <td>26</td>\n      <td>7</td>\n      <td>2002</td>\n      <td>48000</td>\n      <td>Diesel</td>\n      <td>90</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>TOYOTA Corolla 2.0 D4D HATCHB SOL 2/3-Doors</td>\n      <td>13750</td>\n      <td>30</td>\n      <td>3</td>\n      <td>2002</td>\n      <td>38500</td>\n      <td>Diesel</td>\n      <td>90</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1431</th>\n      <td>1438</td>\n      <td>TOYOTA Corolla 1.3 16V HATCHB G6 2/3-Doors</td>\n      <td>7500</td>\n      <td>69</td>\n      <td>12</td>\n      <td>1998</td>\n      <td>20544</td>\n      <td>Petrol</td>\n      <td>86</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1432</th>\n      <td>1439</td>\n      <td>TOYOTA Corolla 1.3 16V HATCHB LINEA TERRA 2/3-...</td>\n      <td>10845</td>\n      <td>72</td>\n      <td>9</td>\n      <td>1998</td>\n      <td>19000</td>\n      <td>Petrol</td>\n      <td>86</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1433</th>\n      <td>1440</td>\n      <td>TOYOTA Corolla 1.3 16V HATCHB LINEA TERRA 2/3-...</td>\n      <td>8500</td>\n      <td>71</td>\n      <td>10</td>\n      <td>1998</td>\n      <td>17016</td>\n      <td>Petrol</td>\n      <td>86</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1434</th>\n      <td>1441</td>\n      <td>TOYOTA Corolla 1.3 16V HATCHB LINEA TERRA 2/3-...</td>\n      <td>7250</td>\n      <td>70</td>\n      <td>11</td>\n      <td>1998</td>\n      <td>16916</td>\n      <td>Petrol</td>\n      <td>86</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1435</th>\n      <td>1442</td>\n      <td>TOYOTA Corolla 1.6 LB LINEA TERRA 4/5-Doors</td>\n      <td>6950</td>\n      <td>76</td>\n      <td>5</td>\n      <td>1998</td>\n      <td>1</td>\n      <td>Petrol</td>\n      <td>110</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1436 rows × 39 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-01-10T05:01:33.803823Z","iopub.execute_input":"2024-01-10T05:01:33.804668Z","iopub.status.idle":"2024-01-10T05:01:33.816607Z","shell.execute_reply.started":"2024-01-10T05:01:33.804626Z","shell.execute_reply":"2024-01-10T05:01:33.815197Z"},"trusted":true},"execution_count":165,"outputs":[{"execution_count":165,"output_type":"execute_result","data":{"text/plain":"Id                   0\nModel                0\nPrice                0\nAge_08_04            0\nMfg_Month            0\nMfg_Year             0\nKM                   0\nFuel_Type            0\nHP                   0\nMet_Color            0\nColor                0\nAutomatic            0\nCC                   0\nDoors                0\nCylinders            0\nGears                0\nQuarterly_Tax        0\nWeight               0\nMfr_Guarantee        0\nBOVAG_Guarantee      0\nGuarantee_Period     0\nABS                  0\nAirbag_1             0\nAirbag_2             0\nAirco                0\nAutomatic_airco      0\nBoardcomputer        0\nCD_Player            0\nCentral_Lock         0\nPowered_Windows      0\nPower_Steering       0\nRadio                0\nMistlamps            0\nSport_Model          0\nBackseat_Divider     0\nMetallic_Rim         0\nRadio_cassette       0\nParking_Assistant    0\nTow_Bar              0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df['Model'].nunique()","metadata":{"execution":{"iopub.status.busy":"2024-01-10T05:01:33.818156Z","iopub.execute_input":"2024-01-10T05:01:33.818547Z","iopub.status.idle":"2024-01-10T05:01:33.826080Z","shell.execute_reply.started":"2024-01-10T05:01:33.818505Z","shell.execute_reply":"2024-01-10T05:01:33.825146Z"},"trusted":true},"execution_count":166,"outputs":[{"execution_count":166,"output_type":"execute_result","data":{"text/plain":"319"},"metadata":{}}]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-01-10T05:01:33.827708Z","iopub.execute_input":"2024-01-10T05:01:33.828076Z","iopub.status.idle":"2024-01-10T05:01:33.848784Z","shell.execute_reply.started":"2024-01-10T05:01:33.828044Z","shell.execute_reply":"2024-01-10T05:01:33.847865Z"},"trusted":true},"execution_count":167,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1436 entries, 0 to 1435\nData columns (total 39 columns):\n #   Column             Non-Null Count  Dtype \n---  ------             --------------  ----- \n 0   Id                 1436 non-null   int64 \n 1   Model              1436 non-null   object\n 2   Price              1436 non-null   int64 \n 3   Age_08_04          1436 non-null   int64 \n 4   Mfg_Month          1436 non-null   int64 \n 5   Mfg_Year           1436 non-null   int64 \n 6   KM                 1436 non-null   int64 \n 7   Fuel_Type          1436 non-null   object\n 8   HP                 1436 non-null   int64 \n 9   Met_Color          1436 non-null   int64 \n 10  Color              1436 non-null   object\n 11  Automatic          1436 non-null   int64 \n 12  CC                 1436 non-null   int64 \n 13  Doors              1436 non-null   int64 \n 14  Cylinders          1436 non-null   int64 \n 15  Gears              1436 non-null   int64 \n 16  Quarterly_Tax      1436 non-null   int64 \n 17  Weight             1436 non-null   int64 \n 18  Mfr_Guarantee      1436 non-null   int64 \n 19  BOVAG_Guarantee    1436 non-null   int64 \n 20  Guarantee_Period   1436 non-null   int64 \n 21  ABS                1436 non-null   int64 \n 22  Airbag_1           1436 non-null   int64 \n 23  Airbag_2           1436 non-null   int64 \n 24  Airco              1436 non-null   int64 \n 25  Automatic_airco    1436 non-null   int64 \n 26  Boardcomputer      1436 non-null   int64 \n 27  CD_Player          1436 non-null   int64 \n 28  Central_Lock       1436 non-null   int64 \n 29  Powered_Windows    1436 non-null   int64 \n 30  Power_Steering     1436 non-null   int64 \n 31  Radio              1436 non-null   int64 \n 32  Mistlamps          1436 non-null   int64 \n 33  Sport_Model        1436 non-null   int64 \n 34  Backseat_Divider   1436 non-null   int64 \n 35  Metallic_Rim       1436 non-null   int64 \n 36  Radio_cassette     1436 non-null   int64 \n 37  Parking_Assistant  1436 non-null   int64 \n 38  Tow_Bar            1436 non-null   int64 \ndtypes: int64(36), object(3)\nmemory usage: 437.7+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"df=df.drop('Model',axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T05:01:33.851042Z","iopub.execute_input":"2024-01-10T05:01:33.852054Z","iopub.status.idle":"2024-01-10T05:01:33.858137Z","shell.execute_reply.started":"2024-01-10T05:01:33.851974Z","shell.execute_reply":"2024-01-10T05:01:33.856952Z"},"trusted":true},"execution_count":168,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nohe=OneHotEncoder(sparse_output=False).set_output(transform='pandas')\nx_=ohe.fit_transform(df[['Fuel_Type']])\ny_=ohe.fit_transform(df[['Color']])\ny_","metadata":{"execution":{"iopub.status.busy":"2024-01-10T05:01:33.859723Z","iopub.execute_input":"2024-01-10T05:01:33.860111Z","iopub.status.idle":"2024-01-10T05:01:33.902824Z","shell.execute_reply.started":"2024-01-10T05:01:33.860077Z","shell.execute_reply":"2024-01-10T05:01:33.901710Z"},"trusted":true},"execution_count":169,"outputs":[{"execution_count":169,"output_type":"execute_result","data":{"text/plain":"      Color_Beige  Color_Black  Color_Blue  Color_Green  Color_Grey  \\\n0             0.0          0.0         1.0          0.0         0.0   \n1             0.0          0.0         0.0          0.0         0.0   \n2             0.0          0.0         1.0          0.0         0.0   \n3             0.0          1.0         0.0          0.0         0.0   \n4             0.0          1.0         0.0          0.0         0.0   \n...           ...          ...         ...          ...         ...   \n1431          0.0          0.0         1.0          0.0         0.0   \n1432          0.0          0.0         0.0          0.0         1.0   \n1433          0.0          0.0         1.0          0.0         0.0   \n1434          0.0          0.0         0.0          0.0         1.0   \n1435          0.0          0.0         0.0          1.0         0.0   \n\n      Color_Red  Color_Silver  Color_Violet  Color_White  Color_Yellow  \n0           0.0           0.0           0.0          0.0           0.0  \n1           0.0           1.0           0.0          0.0           0.0  \n2           0.0           0.0           0.0          0.0           0.0  \n3           0.0           0.0           0.0          0.0           0.0  \n4           0.0           0.0           0.0          0.0           0.0  \n...         ...           ...           ...          ...           ...  \n1431        0.0           0.0           0.0          0.0           0.0  \n1432        0.0           0.0           0.0          0.0           0.0  \n1433        0.0           0.0           0.0          0.0           0.0  \n1434        0.0           0.0           0.0          0.0           0.0  \n1435        0.0           0.0           0.0          0.0           0.0  \n\n[1436 rows x 10 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Color_Beige</th>\n      <th>Color_Black</th>\n      <th>Color_Blue</th>\n      <th>Color_Green</th>\n      <th>Color_Grey</th>\n      <th>Color_Red</th>\n      <th>Color_Silver</th>\n      <th>Color_Violet</th>\n      <th>Color_White</th>\n      <th>Color_Yellow</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1431</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1432</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1433</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1434</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1435</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1436 rows × 10 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df=df.join(x_)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T05:01:33.905400Z","iopub.execute_input":"2024-01-10T05:01:33.906120Z","iopub.status.idle":"2024-01-10T05:01:33.914527Z","shell.execute_reply.started":"2024-01-10T05:01:33.906074Z","shell.execute_reply":"2024-01-10T05:01:33.913095Z"},"trusted":true},"execution_count":170,"outputs":[]},{"cell_type":"code","source":"df=df.join(y_)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T05:01:33.916248Z","iopub.execute_input":"2024-01-10T05:01:33.917004Z","iopub.status.idle":"2024-01-10T05:01:33.923291Z","shell.execute_reply.started":"2024-01-10T05:01:33.916963Z","shell.execute_reply":"2024-01-10T05:01:33.922212Z"},"trusted":true},"execution_count":171,"outputs":[]},{"cell_type":"code","source":"df=df.drop(['Fuel_Type','Color','Id'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T05:01:33.925517Z","iopub.execute_input":"2024-01-10T05:01:33.926121Z","iopub.status.idle":"2024-01-10T05:01:33.935814Z","shell.execute_reply.started":"2024-01-10T05:01:33.926083Z","shell.execute_reply":"2024-01-10T05:01:33.934574Z"},"trusted":true},"execution_count":172,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-01-10T05:01:33.939561Z","iopub.execute_input":"2024-01-10T05:01:33.940317Z","iopub.status.idle":"2024-01-10T05:01:33.955046Z","shell.execute_reply.started":"2024-01-10T05:01:33.940283Z","shell.execute_reply":"2024-01-10T05:01:33.954071Z"},"trusted":true},"execution_count":173,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1436 entries, 0 to 1435\nData columns (total 48 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   Price              1436 non-null   int64  \n 1   Age_08_04          1436 non-null   int64  \n 2   Mfg_Month          1436 non-null   int64  \n 3   Mfg_Year           1436 non-null   int64  \n 4   KM                 1436 non-null   int64  \n 5   HP                 1436 non-null   int64  \n 6   Met_Color          1436 non-null   int64  \n 7   Automatic          1436 non-null   int64  \n 8   CC                 1436 non-null   int64  \n 9   Doors              1436 non-null   int64  \n 10  Cylinders          1436 non-null   int64  \n 11  Gears              1436 non-null   int64  \n 12  Quarterly_Tax      1436 non-null   int64  \n 13  Weight             1436 non-null   int64  \n 14  Mfr_Guarantee      1436 non-null   int64  \n 15  BOVAG_Guarantee    1436 non-null   int64  \n 16  Guarantee_Period   1436 non-null   int64  \n 17  ABS                1436 non-null   int64  \n 18  Airbag_1           1436 non-null   int64  \n 19  Airbag_2           1436 non-null   int64  \n 20  Airco              1436 non-null   int64  \n 21  Automatic_airco    1436 non-null   int64  \n 22  Boardcomputer      1436 non-null   int64  \n 23  CD_Player          1436 non-null   int64  \n 24  Central_Lock       1436 non-null   int64  \n 25  Powered_Windows    1436 non-null   int64  \n 26  Power_Steering     1436 non-null   int64  \n 27  Radio              1436 non-null   int64  \n 28  Mistlamps          1436 non-null   int64  \n 29  Sport_Model        1436 non-null   int64  \n 30  Backseat_Divider   1436 non-null   int64  \n 31  Metallic_Rim       1436 non-null   int64  \n 32  Radio_cassette     1436 non-null   int64  \n 33  Parking_Assistant  1436 non-null   int64  \n 34  Tow_Bar            1436 non-null   int64  \n 35  Fuel_Type_CNG      1436 non-null   float64\n 36  Fuel_Type_Diesel   1436 non-null   float64\n 37  Fuel_Type_Petrol   1436 non-null   float64\n 38  Color_Beige        1436 non-null   float64\n 39  Color_Black        1436 non-null   float64\n 40  Color_Blue         1436 non-null   float64\n 41  Color_Green        1436 non-null   float64\n 42  Color_Grey         1436 non-null   float64\n 43  Color_Red          1436 non-null   float64\n 44  Color_Silver       1436 non-null   float64\n 45  Color_Violet       1436 non-null   float64\n 46  Color_White        1436 non-null   float64\n 47  Color_Yellow       1436 non-null   float64\ndtypes: float64(13), int64(35)\nmemory usage: 538.6 KB\n","output_type":"stream"}]},{"cell_type":"code","source":"df.corr()","metadata":{"execution":{"iopub.status.busy":"2024-01-10T05:01:33.956311Z","iopub.execute_input":"2024-01-10T05:01:33.956852Z","iopub.status.idle":"2024-01-10T05:01:34.046013Z","shell.execute_reply.started":"2024-01-10T05:01:33.956820Z","shell.execute_reply":"2024-01-10T05:01:34.045015Z"},"trusted":true},"execution_count":174,"outputs":[{"execution_count":174,"output_type":"execute_result","data":{"text/plain":"                      Price  Age_08_04  Mfg_Month  Mfg_Year        KM  \\\nPrice              1.000000  -0.876590  -0.018138  0.885159 -0.569960   \nAge_08_04         -0.876590   1.000000  -0.123255 -0.983661  0.505672   \nMfg_Month         -0.018138  -0.123255   1.000000 -0.057416 -0.020630   \nMfg_Year           0.885159  -0.983661  -0.057416  1.000000 -0.504974   \nKM                -0.569960   0.505672  -0.020630 -0.504974  1.000000   \nHP                 0.314990  -0.156622  -0.039312  0.164697 -0.333538   \nMet_Color          0.108905  -0.108150   0.030266  0.103310 -0.080503   \nAutomatic          0.033081   0.031717   0.009146 -0.033567 -0.081854   \nCC                 0.126389  -0.098084   0.037387  0.091892  0.102683   \nDoors              0.185326  -0.148359  -0.012069  0.151442 -0.036197   \nCylinders               NaN        NaN        NaN       NaN       NaN   \nGears              0.063104  -0.005364  -0.013063  0.007766  0.015023   \nQuarterly_Tax      0.219197  -0.198431   0.031373  0.193934  0.278165   \nWeight             0.581198  -0.470253  -0.002167  0.473478 -0.028598   \nMfr_Guarantee      0.197802  -0.164658  -0.005771  0.166697 -0.212851   \nBOVAG_Guarantee    0.028133   0.006865  -0.003863 -0.006206  0.001438   \nGuarantee_Period   0.146627  -0.152563   0.029010  0.148218 -0.138942   \nABS                0.306138  -0.412887   0.072532  0.402215 -0.177203   \nAirbag_1           0.093588  -0.105406   0.003756  0.105359 -0.018012   \nAirbag_2           0.248974  -0.329017   0.076749  0.317075 -0.139275   \nAirco              0.429259  -0.403600   0.057088  0.395674 -0.133057   \nAutomatic_airco    0.588262  -0.426259  -0.049017  0.437718 -0.258221   \nBoardcomputer      0.601292  -0.719449   0.017715  0.720567 -0.353862   \nCD_Player          0.481374  -0.510895  -0.016736  0.517008 -0.266826   \nCentral_Lock       0.343458  -0.279631   0.010055  0.279490 -0.125177   \nPowered_Windows    0.356518  -0.283856   0.025185  0.280996 -0.156242   \nPower_Steering     0.064275  -0.069192  -0.055495  0.079676  0.007397   \nRadio             -0.041887   0.013791   0.031601 -0.019607  0.013661   \nMistlamps          0.222083  -0.126895  -0.033504  0.133737 -0.074327   \nSport_Model        0.164121  -0.110988   0.052789  0.102080 -0.044784   \nBackseat_Divider   0.102569  -0.116751   0.023245  0.113237 -0.045658   \nMetallic_Rim       0.108564  -0.040045   0.023506  0.036022 -0.013599   \nRadio_cassette    -0.043179   0.012857   0.032576 -0.018844  0.015770   \nParking_Assistant  0.044375  -0.048172   0.007113  0.047171 -0.064318   \nTow_Bar           -0.172369   0.188720  -0.042170 -0.182206  0.084153   \nFuel_Type_CNG     -0.039536   0.002389   0.001289 -0.002637  0.144016   \nFuel_Type_Diesel   0.054084  -0.097740   0.051501  0.088986  0.403060   \nFuel_Type_Petrol  -0.038516   0.092611  -0.049646 -0.084162 -0.433160   \nColor_Beige       -0.022684   0.023098   0.028902 -0.028480 -0.006720   \nColor_Black        0.034896  -0.019399   0.008067  0.018052  0.039012   \nColor_Blue         0.014431  -0.032955   0.057279  0.022763 -0.003840   \nColor_Green       -0.104963   0.103530  -0.030403 -0.098638 -0.017439   \nColor_Grey         0.169947  -0.137660  -0.031214  0.144152 -0.112194   \nColor_Red         -0.103803   0.098365   0.008646 -0.100526  0.052190   \nColor_Silver       0.028562  -0.028550  -0.017838  0.031958  0.006113   \nColor_Violet      -0.016848   0.017916  -0.028353 -0.012880  0.018810   \nColor_White       -0.103360   0.050933  -0.001445 -0.050977  0.130403   \nColor_Yellow       0.022726  -0.040883   0.001609  0.040837 -0.037995   \n\n                         HP  Met_Color  Automatic        CC     Doors  ...  \\\nPrice              0.314990   0.108905   0.033081  0.126389  0.185326  ...   \nAge_08_04         -0.156622  -0.108150   0.031717 -0.098084 -0.148359  ...   \nMfg_Month         -0.039312   0.030266   0.009146  0.037387 -0.012069  ...   \nMfg_Year           0.164697   0.103310  -0.033567  0.091892  0.151442  ...   \nKM                -0.333538  -0.080503  -0.081854  0.102683 -0.036197  ...   \nHP                 1.000000   0.058712   0.013144  0.035856  0.092424  ...   \nMet_Color          0.058712   1.000000  -0.019335  0.031812  0.085243  ...   \nAutomatic          0.013144  -0.019335   1.000000  0.066740 -0.027654  ...   \nCC                 0.035856   0.031812   0.066740  1.000000  0.079903  ...   \nDoors              0.092424   0.085243  -0.027654  0.079903  1.000000  ...   \nCylinders               NaN        NaN        NaN       NaN       NaN  ...   \nGears              0.209477   0.018601  -0.098555  0.014629 -0.160141  ...   \nQuarterly_Tax     -0.298432   0.011326  -0.055371  0.306996  0.109363  ...   \nWeight             0.089614   0.057929   0.057249  0.335637  0.302618  ...   \nMfr_Guarantee      0.140026   0.154850   0.026194 -0.057407  0.037689  ...   \nBOVAG_Guarantee    0.022701   0.010783   0.023393 -0.081725 -0.014311  ...   \nGuarantee_Period   0.076163   0.009295  -0.002256 -0.017683  0.053654  ...   \nABS                0.057832   0.022298  -0.016128  0.037806  0.063733  ...   \nAirbag_1           0.025137   0.100055  -0.011895  0.022678  0.053828  ...   \nAirbag_2           0.017644   0.038416   0.001171  0.024738  0.021734  ...   \nAirco              0.241134   0.114190  -0.028353  0.119888  0.170544  ...   \nAutomatic_airco    0.244957   0.027977   0.059057  0.162669  0.054809  ...   \nBoardcomputer      0.129715   0.089886  -0.037069  0.009312  0.089606  ...   \nCD_Player          0.102300   0.198220  -0.010967  0.057787  0.094653  ...   \nCentral_Lock       0.250122   0.153307  -0.002502  0.072634  0.132092  ...   \nPowered_Windows    0.265593   0.145147  -0.005864  0.055299  0.107626  ...   \nPower_Steering     0.048850   0.086544  -0.004469  0.032933  0.059792  ...   \nRadio              0.020998   0.072756  -0.014600 -0.000361 -0.008318  ...   \nMistlamps          0.210571   0.023821   0.003077  0.017326  0.064705  ...   \nSport_Model       -0.006027   0.003779   0.013175 -0.035195 -0.129881  ...   \nBackseat_Divider   0.010908   0.037741  -0.018876 -0.055711 -0.022542  ...   \nMetallic_Rim       0.206784   0.053829  -0.078095  0.003236 -0.039555  ...   \nRadio_cassette     0.019919   0.071530  -0.014150 -0.000470 -0.008265  ...   \nParking_Assistant  0.029990   0.036691   0.159985  0.002883  0.025893  ...   \nTow_Bar            0.068271   0.148536   0.018786  0.002725  0.102292  ...   \nFuel_Type_CNG      0.062109   0.021009   0.001486  0.005941  0.009680  ...   \nFuel_Type_Diesel  -0.533453  -0.012420  -0.084490  0.327723  0.025495  ...   \nFuel_Type_Petrol   0.489110   0.004872   0.080249 -0.315170 -0.027589  ...   \nColor_Beige        0.025963   0.031764  -0.011114  0.002496 -0.033636  ...   \nColor_Black       -0.002040   0.061789  -0.041491 -0.007964 -0.099880  ...   \nColor_Blue        -0.027949   0.044972  -0.005847  0.029421 -0.044962  ...   \nColor_Green        0.021242   0.084797   0.056846 -0.010579  0.060186  ...   \nColor_Grey         0.021349   0.072634   0.001725 -0.009867  0.059174  ...   \nColor_Red          0.011700  -0.261828  -0.026799 -0.008865 -0.000541  ...   \nColor_Silver       0.014300   0.104902   0.045772 -0.001351  0.028644  ...   \nColor_Violet      -0.012359   0.036691  -0.012837 -0.015804  0.012019  ...   \nColor_White       -0.093928  -0.203738  -0.036079  0.027304 -0.010245  ...   \nColor_Yellow      -0.000516   0.031764  -0.011114 -0.011884  0.014409  ...   \n\n                   Color_Beige  Color_Black  Color_Blue  Color_Green  \\\nPrice                -0.022684     0.034896    0.014431    -0.104963   \nAge_08_04             0.023098    -0.019399   -0.032955     0.103530   \nMfg_Month             0.028902     0.008067    0.057279    -0.030403   \nMfg_Year             -0.028480     0.018052    0.022763    -0.098638   \nKM                   -0.006720     0.039012   -0.003840    -0.017439   \nHP                    0.025963    -0.002040   -0.027949     0.021242   \nMet_Color             0.031764     0.061789    0.044972     0.084797   \nAutomatic            -0.011114    -0.041491   -0.005847     0.056846   \nCC                    0.002496    -0.007964    0.029421    -0.010579   \nDoors                -0.033636    -0.099880   -0.044962     0.060186   \nCylinders                  NaN          NaN         NaN          NaN   \nGears                 0.074509     0.129995   -0.023121    -0.049470   \nQuarterly_Tax        -0.014233     0.005516   -0.021361    -0.049610   \nWeight               -0.019528     0.018792   -0.029244    -0.039096   \nMfr_Guarantee         0.023931     0.011640   -0.024494     0.031126   \nBOVAG_Guarantee       0.015627     0.026492   -0.042577    -0.019087   \nGuarantee_Period     -0.012396    -0.006645   -0.015573     0.010662   \nABS                  -0.017228    -0.038706    0.003667    -0.024521   \nAirbag_1              0.007942     0.019306    0.002880     0.027934   \nAirbag_2             -0.005742     0.018039    0.001705    -0.047623   \nAirco                 0.014489     0.036523   -0.017036     0.000625   \nAutomatic_airco      -0.011187     0.037567    0.038221    -0.095616   \nBoardcomputer         0.003891     0.030309    0.037010    -0.092482   \nCD_Player            -0.024205     0.040858    0.051325    -0.080013   \nCentral_Lock          0.008027     0.083948   -0.021863    -0.010257   \nPowered_Windows       0.009655     0.073002   -0.000140    -0.021959   \nPower_Steering        0.006908    -0.010333    0.039214    -0.014375   \nRadio                 0.024227    -0.051836   -0.001911     0.031885   \nMistlamps             0.007997     0.140416    0.001119    -0.006779   \nSport_Model           0.003314     0.088026    0.030789    -0.084499   \nBackseat_Divider     -0.011259     0.067718    0.029273    -0.020418   \nMetallic_Rim          0.052380     0.146851    0.008937    -0.038532   \nRadio_cassette        0.024365    -0.051166   -0.005901     0.027306   \nParking_Assistant    -0.002418    -0.020701    0.007032     0.014203   \nTow_Bar               0.039715    -0.036946    0.009251     0.012396   \nFuel_Type_CNG        -0.005008     0.032969   -0.021855    -0.010805   \nFuel_Type_Diesel     -0.015916    -0.010682   -0.020009    -0.035805   \nFuel_Type_Petrol      0.016878    -0.000774    0.026402     0.037816   \nColor_Beige           1.000000    -0.017921   -0.022668    -0.019462   \nColor_Black          -0.017921     1.000000   -0.194049    -0.166601   \nColor_Blue           -0.022668    -0.194049    1.000000    -0.210728   \nColor_Green          -0.019462    -0.166601   -0.210728     1.000000   \nColor_Grey           -0.023563    -0.201705   -0.255131    -0.219043   \nColor_Red            -0.022418    -0.191911   -0.242743    -0.208407   \nColor_Silver         -0.013942    -0.119348   -0.150960    -0.129607   \nColor_Violet         -0.002418    -0.020701   -0.026184    -0.022480   \nColor_White          -0.006796    -0.058180   -0.073590    -0.063181   \nColor_Yellow         -0.002094    -0.017921   -0.022668    -0.019462   \n\n                   Color_Grey  Color_Red  Color_Silver  Color_Violet  \\\nPrice                0.169947  -0.103803      0.028562     -0.016848   \nAge_08_04           -0.137660   0.098365     -0.028550      0.017916   \nMfg_Month           -0.031214   0.008646     -0.017838     -0.028353   \nMfg_Year             0.144152  -0.100526      0.031958     -0.012880   \nKM                  -0.112194   0.052190      0.006113      0.018810   \nHP                   0.021349   0.011700      0.014300     -0.012359   \nMet_Color            0.072634  -0.261828      0.104902      0.036691   \nAutomatic            0.001725  -0.026799      0.045772     -0.012837   \nCC                  -0.009867  -0.008865     -0.001351     -0.015804   \nDoors                0.059174  -0.000541      0.028644      0.012019   \nCylinders                 NaN        NaN           NaN           NaN   \nGears               -0.026921   0.034076     -0.069296     -0.007422   \nQuarterly_Tax        0.047443   0.016672     -0.023992     -0.023296   \nWeight               0.124445  -0.078332     -0.003136     -0.036367   \nMfr_Guarantee        0.013045   0.000599     -0.009932      0.036600   \nBOVAG_Guarantee      0.058408   0.040562     -0.083753      0.018050   \nGuarantee_Period    -0.066192  -0.036127      0.159743      0.117373   \nABS                  0.062248   0.013042     -0.020713     -0.008596   \nAirbag_1             0.069081  -0.082309     -0.021222     -0.069241   \nAirbag_2             0.112473   0.019886     -0.107062     -0.085352   \nAirco                0.020481  -0.025817      0.019886     -0.027313   \nAutomatic_airco      0.111398  -0.058680     -0.031196     -0.012922   \nBoardcomputer        0.166397  -0.107833     -0.038010     -0.034153   \nCD_Player            0.087678  -0.075849     -0.004090     -0.027959   \nCentral_Lock         0.042967  -0.079503      0.036587     -0.008576   \nPowered_Windows      0.033948  -0.075416      0.047515     -0.006602   \nPower_Steering       0.019792  -0.021553     -0.021681     -0.081536   \nRadio               -0.058191  -0.018229      0.121285      0.015520   \nMistlamps            0.002560  -0.062261     -0.047725     -0.031081   \nSport_Model          0.043519  -0.024760     -0.052408     -0.034611   \nBackseat_Divider     0.086097  -0.021425     -0.154138     -0.096756   \nMetallic_Rim        -0.062012  -0.034578     -0.024621      0.005929   \nRadio_cassette      -0.057289  -0.012300      0.122127      0.015655   \nParking_Assistant   -0.027217   0.007545      0.031285     -0.002793   \nTow_Bar             -0.074992   0.081668      0.022870     -0.003287   \nFuel_Type_CNG       -0.040548   0.044143     -0.010260     -0.005785   \nFuel_Type_Diesel     0.024869  -0.011399      0.014741     -0.018384   \nFuel_Type_Petrol    -0.010259  -0.003810     -0.010670      0.019496   \nColor_Beige         -0.023563  -0.022418     -0.013942     -0.002418   \nColor_Black         -0.201705  -0.191911     -0.119348     -0.020701   \nColor_Blue          -0.255131  -0.242743     -0.150960     -0.026184   \nColor_Green         -0.219043  -0.208407     -0.129607     -0.022480   \nColor_Grey           1.000000  -0.252321     -0.156916     -0.027217   \nColor_Red           -0.252321   1.000000     -0.149297     -0.025896   \nColor_Silver        -0.156916  -0.149297      1.000000     -0.016104   \nColor_Violet        -0.027217  -0.025896     -0.016104      1.000000   \nColor_White         -0.076494  -0.072780     -0.045261     -0.007851   \nColor_Yellow        -0.023563  -0.022418     -0.013942     -0.002418   \n\n                   Color_White  Color_Yellow  \nPrice                -0.103360      0.022726  \nAge_08_04             0.050933     -0.040883  \nMfg_Month            -0.001445      0.001609  \nMfg_Year             -0.050977      0.040837  \nKM                    0.130403     -0.037995  \nHP                   -0.093928     -0.000516  \nMet_Color            -0.203738      0.031764  \nAutomatic            -0.036079     -0.011114  \nCC                    0.027304     -0.011884  \nDoors                -0.010245      0.014409  \nCylinders                  NaN           NaN  \nGears                -0.020859     -0.006425  \nQuarterly_Tax         0.051885     -0.008297  \nWeight                0.027567     -0.015181  \nMfr_Guarantee        -0.094457      0.054947  \nBOVAG_Guarantee      -0.027602      0.015627  \nGuarantee_Period     -0.025915     -0.012396  \nABS                  -0.027234      0.021917  \nAirbag_1             -0.031090      0.007942  \nAirbag_2             -0.068600      0.028332  \nAirco                -0.064782      0.014489  \nAutomatic_airco      -0.036317     -0.011187  \nBoardcomputer        -0.043429      0.037349  \nCD_Player            -0.055395      0.012694  \nCentral_Lock         -0.077499     -0.022875  \nPowered_Windows      -0.090988     -0.021086  \nPower_Steering        0.022425      0.006908  \nRadio                 0.019888     -0.018937  \nMistlamps            -0.054455      0.007997  \nSport_Model          -0.013636     -0.029964  \nBackseat_Divider     -0.044146      0.024993  \nMetallic_Rim          0.007757      0.052380  \nRadio_cassette        0.020220     -0.018884  \nParking_Assistant    -0.007851     -0.002418  \nTow_Bar              -0.027957     -0.028381  \nFuel_Type_CNG         0.028043     -0.005008  \nFuel_Type_Diesel      0.118189     -0.015916  \nFuel_Type_Petrol     -0.122290      0.016878  \nColor_Beige          -0.006796     -0.002094  \nColor_Black          -0.058180     -0.017921  \nColor_Blue           -0.073590     -0.022668  \nColor_Green          -0.063181     -0.019462  \nColor_Grey           -0.076494     -0.023563  \nColor_Red            -0.072780     -0.022418  \nColor_Silver         -0.045261     -0.013942  \nColor_Violet         -0.007851     -0.002418  \nColor_White           1.000000     -0.006796  \nColor_Yellow         -0.006796      1.000000  \n\n[48 rows x 48 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Price</th>\n      <th>Age_08_04</th>\n      <th>Mfg_Month</th>\n      <th>Mfg_Year</th>\n      <th>KM</th>\n      <th>HP</th>\n      <th>Met_Color</th>\n      <th>Automatic</th>\n      <th>CC</th>\n      <th>Doors</th>\n      <th>...</th>\n      <th>Color_Beige</th>\n      <th>Color_Black</th>\n      <th>Color_Blue</th>\n      <th>Color_Green</th>\n      <th>Color_Grey</th>\n      <th>Color_Red</th>\n      <th>Color_Silver</th>\n      <th>Color_Violet</th>\n      <th>Color_White</th>\n      <th>Color_Yellow</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Price</th>\n      <td>1.000000</td>\n      <td>-0.876590</td>\n      <td>-0.018138</td>\n      <td>0.885159</td>\n      <td>-0.569960</td>\n      <td>0.314990</td>\n      <td>0.108905</td>\n      <td>0.033081</td>\n      <td>0.126389</td>\n      <td>0.185326</td>\n      <td>...</td>\n      <td>-0.022684</td>\n      <td>0.034896</td>\n      <td>0.014431</td>\n      <td>-0.104963</td>\n      <td>0.169947</td>\n      <td>-0.103803</td>\n      <td>0.028562</td>\n      <td>-0.016848</td>\n      <td>-0.103360</td>\n      <td>0.022726</td>\n    </tr>\n    <tr>\n      <th>Age_08_04</th>\n      <td>-0.876590</td>\n      <td>1.000000</td>\n      <td>-0.123255</td>\n      <td>-0.983661</td>\n      <td>0.505672</td>\n      <td>-0.156622</td>\n      <td>-0.108150</td>\n      <td>0.031717</td>\n      <td>-0.098084</td>\n      <td>-0.148359</td>\n      <td>...</td>\n      <td>0.023098</td>\n      <td>-0.019399</td>\n      <td>-0.032955</td>\n      <td>0.103530</td>\n      <td>-0.137660</td>\n      <td>0.098365</td>\n      <td>-0.028550</td>\n      <td>0.017916</td>\n      <td>0.050933</td>\n      <td>-0.040883</td>\n    </tr>\n    <tr>\n      <th>Mfg_Month</th>\n      <td>-0.018138</td>\n      <td>-0.123255</td>\n      <td>1.000000</td>\n      <td>-0.057416</td>\n      <td>-0.020630</td>\n      <td>-0.039312</td>\n      <td>0.030266</td>\n      <td>0.009146</td>\n      <td>0.037387</td>\n      <td>-0.012069</td>\n      <td>...</td>\n      <td>0.028902</td>\n      <td>0.008067</td>\n      <td>0.057279</td>\n      <td>-0.030403</td>\n      <td>-0.031214</td>\n      <td>0.008646</td>\n      <td>-0.017838</td>\n      <td>-0.028353</td>\n      <td>-0.001445</td>\n      <td>0.001609</td>\n    </tr>\n    <tr>\n      <th>Mfg_Year</th>\n      <td>0.885159</td>\n      <td>-0.983661</td>\n      <td>-0.057416</td>\n      <td>1.000000</td>\n      <td>-0.504974</td>\n      <td>0.164697</td>\n      <td>0.103310</td>\n      <td>-0.033567</td>\n      <td>0.091892</td>\n      <td>0.151442</td>\n      <td>...</td>\n      <td>-0.028480</td>\n      <td>0.018052</td>\n      <td>0.022763</td>\n      <td>-0.098638</td>\n      <td>0.144152</td>\n      <td>-0.100526</td>\n      <td>0.031958</td>\n      <td>-0.012880</td>\n      <td>-0.050977</td>\n      <td>0.040837</td>\n    </tr>\n    <tr>\n      <th>KM</th>\n      <td>-0.569960</td>\n      <td>0.505672</td>\n      <td>-0.020630</td>\n      <td>-0.504974</td>\n      <td>1.000000</td>\n      <td>-0.333538</td>\n      <td>-0.080503</td>\n      <td>-0.081854</td>\n      <td>0.102683</td>\n      <td>-0.036197</td>\n      <td>...</td>\n      <td>-0.006720</td>\n      <td>0.039012</td>\n      <td>-0.003840</td>\n      <td>-0.017439</td>\n      <td>-0.112194</td>\n      <td>0.052190</td>\n      <td>0.006113</td>\n      <td>0.018810</td>\n      <td>0.130403</td>\n      <td>-0.037995</td>\n    </tr>\n    <tr>\n      <th>HP</th>\n      <td>0.314990</td>\n      <td>-0.156622</td>\n      <td>-0.039312</td>\n      <td>0.164697</td>\n      <td>-0.333538</td>\n      <td>1.000000</td>\n      <td>0.058712</td>\n      <td>0.013144</td>\n      <td>0.035856</td>\n      <td>0.092424</td>\n      <td>...</td>\n      <td>0.025963</td>\n      <td>-0.002040</td>\n      <td>-0.027949</td>\n      <td>0.021242</td>\n      <td>0.021349</td>\n      <td>0.011700</td>\n      <td>0.014300</td>\n      <td>-0.012359</td>\n      <td>-0.093928</td>\n      <td>-0.000516</td>\n    </tr>\n    <tr>\n      <th>Met_Color</th>\n      <td>0.108905</td>\n      <td>-0.108150</td>\n      <td>0.030266</td>\n      <td>0.103310</td>\n      <td>-0.080503</td>\n      <td>0.058712</td>\n      <td>1.000000</td>\n      <td>-0.019335</td>\n      <td>0.031812</td>\n      <td>0.085243</td>\n      <td>...</td>\n      <td>0.031764</td>\n      <td>0.061789</td>\n      <td>0.044972</td>\n      <td>0.084797</td>\n      <td>0.072634</td>\n      <td>-0.261828</td>\n      <td>0.104902</td>\n      <td>0.036691</td>\n      <td>-0.203738</td>\n      <td>0.031764</td>\n    </tr>\n    <tr>\n      <th>Automatic</th>\n      <td>0.033081</td>\n      <td>0.031717</td>\n      <td>0.009146</td>\n      <td>-0.033567</td>\n      <td>-0.081854</td>\n      <td>0.013144</td>\n      <td>-0.019335</td>\n      <td>1.000000</td>\n      <td>0.066740</td>\n      <td>-0.027654</td>\n      <td>...</td>\n      <td>-0.011114</td>\n      <td>-0.041491</td>\n      <td>-0.005847</td>\n      <td>0.056846</td>\n      <td>0.001725</td>\n      <td>-0.026799</td>\n      <td>0.045772</td>\n      <td>-0.012837</td>\n      <td>-0.036079</td>\n      <td>-0.011114</td>\n    </tr>\n    <tr>\n      <th>CC</th>\n      <td>0.126389</td>\n      <td>-0.098084</td>\n      <td>0.037387</td>\n      <td>0.091892</td>\n      <td>0.102683</td>\n      <td>0.035856</td>\n      <td>0.031812</td>\n      <td>0.066740</td>\n      <td>1.000000</td>\n      <td>0.079903</td>\n      <td>...</td>\n      <td>0.002496</td>\n      <td>-0.007964</td>\n      <td>0.029421</td>\n      <td>-0.010579</td>\n      <td>-0.009867</td>\n      <td>-0.008865</td>\n      <td>-0.001351</td>\n      <td>-0.015804</td>\n      <td>0.027304</td>\n      <td>-0.011884</td>\n    </tr>\n    <tr>\n      <th>Doors</th>\n      <td>0.185326</td>\n      <td>-0.148359</td>\n      <td>-0.012069</td>\n      <td>0.151442</td>\n      <td>-0.036197</td>\n      <td>0.092424</td>\n      <td>0.085243</td>\n      <td>-0.027654</td>\n      <td>0.079903</td>\n      <td>1.000000</td>\n      <td>...</td>\n      <td>-0.033636</td>\n      <td>-0.099880</td>\n      <td>-0.044962</td>\n      <td>0.060186</td>\n      <td>0.059174</td>\n      <td>-0.000541</td>\n      <td>0.028644</td>\n      <td>0.012019</td>\n      <td>-0.010245</td>\n      <td>0.014409</td>\n    </tr>\n    <tr>\n      <th>Cylinders</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Gears</th>\n      <td>0.063104</td>\n      <td>-0.005364</td>\n      <td>-0.013063</td>\n      <td>0.007766</td>\n      <td>0.015023</td>\n      <td>0.209477</td>\n      <td>0.018601</td>\n      <td>-0.098555</td>\n      <td>0.014629</td>\n      <td>-0.160141</td>\n      <td>...</td>\n      <td>0.074509</td>\n      <td>0.129995</td>\n      <td>-0.023121</td>\n      <td>-0.049470</td>\n      <td>-0.026921</td>\n      <td>0.034076</td>\n      <td>-0.069296</td>\n      <td>-0.007422</td>\n      <td>-0.020859</td>\n      <td>-0.006425</td>\n    </tr>\n    <tr>\n      <th>Quarterly_Tax</th>\n      <td>0.219197</td>\n      <td>-0.198431</td>\n      <td>0.031373</td>\n      <td>0.193934</td>\n      <td>0.278165</td>\n      <td>-0.298432</td>\n      <td>0.011326</td>\n      <td>-0.055371</td>\n      <td>0.306996</td>\n      <td>0.109363</td>\n      <td>...</td>\n      <td>-0.014233</td>\n      <td>0.005516</td>\n      <td>-0.021361</td>\n      <td>-0.049610</td>\n      <td>0.047443</td>\n      <td>0.016672</td>\n      <td>-0.023992</td>\n      <td>-0.023296</td>\n      <td>0.051885</td>\n      <td>-0.008297</td>\n    </tr>\n    <tr>\n      <th>Weight</th>\n      <td>0.581198</td>\n      <td>-0.470253</td>\n      <td>-0.002167</td>\n      <td>0.473478</td>\n      <td>-0.028598</td>\n      <td>0.089614</td>\n      <td>0.057929</td>\n      <td>0.057249</td>\n      <td>0.335637</td>\n      <td>0.302618</td>\n      <td>...</td>\n      <td>-0.019528</td>\n      <td>0.018792</td>\n      <td>-0.029244</td>\n      <td>-0.039096</td>\n      <td>0.124445</td>\n      <td>-0.078332</td>\n      <td>-0.003136</td>\n      <td>-0.036367</td>\n      <td>0.027567</td>\n      <td>-0.015181</td>\n    </tr>\n    <tr>\n      <th>Mfr_Guarantee</th>\n      <td>0.197802</td>\n      <td>-0.164658</td>\n      <td>-0.005771</td>\n      <td>0.166697</td>\n      <td>-0.212851</td>\n      <td>0.140026</td>\n      <td>0.154850</td>\n      <td>0.026194</td>\n      <td>-0.057407</td>\n      <td>0.037689</td>\n      <td>...</td>\n      <td>0.023931</td>\n      <td>0.011640</td>\n      <td>-0.024494</td>\n      <td>0.031126</td>\n      <td>0.013045</td>\n      <td>0.000599</td>\n      <td>-0.009932</td>\n      <td>0.036600</td>\n      <td>-0.094457</td>\n      <td>0.054947</td>\n    </tr>\n    <tr>\n      <th>BOVAG_Guarantee</th>\n      <td>0.028133</td>\n      <td>0.006865</td>\n      <td>-0.003863</td>\n      <td>-0.006206</td>\n      <td>0.001438</td>\n      <td>0.022701</td>\n      <td>0.010783</td>\n      <td>0.023393</td>\n      <td>-0.081725</td>\n      <td>-0.014311</td>\n      <td>...</td>\n      <td>0.015627</td>\n      <td>0.026492</td>\n      <td>-0.042577</td>\n      <td>-0.019087</td>\n      <td>0.058408</td>\n      <td>0.040562</td>\n      <td>-0.083753</td>\n      <td>0.018050</td>\n      <td>-0.027602</td>\n      <td>0.015627</td>\n    </tr>\n    <tr>\n      <th>Guarantee_Period</th>\n      <td>0.146627</td>\n      <td>-0.152563</td>\n      <td>0.029010</td>\n      <td>0.148218</td>\n      <td>-0.138942</td>\n      <td>0.076163</td>\n      <td>0.009295</td>\n      <td>-0.002256</td>\n      <td>-0.017683</td>\n      <td>0.053654</td>\n      <td>...</td>\n      <td>-0.012396</td>\n      <td>-0.006645</td>\n      <td>-0.015573</td>\n      <td>0.010662</td>\n      <td>-0.066192</td>\n      <td>-0.036127</td>\n      <td>0.159743</td>\n      <td>0.117373</td>\n      <td>-0.025915</td>\n      <td>-0.012396</td>\n    </tr>\n    <tr>\n      <th>ABS</th>\n      <td>0.306138</td>\n      <td>-0.412887</td>\n      <td>0.072532</td>\n      <td>0.402215</td>\n      <td>-0.177203</td>\n      <td>0.057832</td>\n      <td>0.022298</td>\n      <td>-0.016128</td>\n      <td>0.037806</td>\n      <td>0.063733</td>\n      <td>...</td>\n      <td>-0.017228</td>\n      <td>-0.038706</td>\n      <td>0.003667</td>\n      <td>-0.024521</td>\n      <td>0.062248</td>\n      <td>0.013042</td>\n      <td>-0.020713</td>\n      <td>-0.008596</td>\n      <td>-0.027234</td>\n      <td>0.021917</td>\n    </tr>\n    <tr>\n      <th>Airbag_1</th>\n      <td>0.093588</td>\n      <td>-0.105406</td>\n      <td>0.003756</td>\n      <td>0.105359</td>\n      <td>-0.018012</td>\n      <td>0.025137</td>\n      <td>0.100055</td>\n      <td>-0.011895</td>\n      <td>0.022678</td>\n      <td>0.053828</td>\n      <td>...</td>\n      <td>0.007942</td>\n      <td>0.019306</td>\n      <td>0.002880</td>\n      <td>0.027934</td>\n      <td>0.069081</td>\n      <td>-0.082309</td>\n      <td>-0.021222</td>\n      <td>-0.069241</td>\n      <td>-0.031090</td>\n      <td>0.007942</td>\n    </tr>\n    <tr>\n      <th>Airbag_2</th>\n      <td>0.248974</td>\n      <td>-0.329017</td>\n      <td>0.076749</td>\n      <td>0.317075</td>\n      <td>-0.139275</td>\n      <td>0.017644</td>\n      <td>0.038416</td>\n      <td>0.001171</td>\n      <td>0.024738</td>\n      <td>0.021734</td>\n      <td>...</td>\n      <td>-0.005742</td>\n      <td>0.018039</td>\n      <td>0.001705</td>\n      <td>-0.047623</td>\n      <td>0.112473</td>\n      <td>0.019886</td>\n      <td>-0.107062</td>\n      <td>-0.085352</td>\n      <td>-0.068600</td>\n      <td>0.028332</td>\n    </tr>\n    <tr>\n      <th>Airco</th>\n      <td>0.429259</td>\n      <td>-0.403600</td>\n      <td>0.057088</td>\n      <td>0.395674</td>\n      <td>-0.133057</td>\n      <td>0.241134</td>\n      <td>0.114190</td>\n      <td>-0.028353</td>\n      <td>0.119888</td>\n      <td>0.170544</td>\n      <td>...</td>\n      <td>0.014489</td>\n      <td>0.036523</td>\n      <td>-0.017036</td>\n      <td>0.000625</td>\n      <td>0.020481</td>\n      <td>-0.025817</td>\n      <td>0.019886</td>\n      <td>-0.027313</td>\n      <td>-0.064782</td>\n      <td>0.014489</td>\n    </tr>\n    <tr>\n      <th>Automatic_airco</th>\n      <td>0.588262</td>\n      <td>-0.426259</td>\n      <td>-0.049017</td>\n      <td>0.437718</td>\n      <td>-0.258221</td>\n      <td>0.244957</td>\n      <td>0.027977</td>\n      <td>0.059057</td>\n      <td>0.162669</td>\n      <td>0.054809</td>\n      <td>...</td>\n      <td>-0.011187</td>\n      <td>0.037567</td>\n      <td>0.038221</td>\n      <td>-0.095616</td>\n      <td>0.111398</td>\n      <td>-0.058680</td>\n      <td>-0.031196</td>\n      <td>-0.012922</td>\n      <td>-0.036317</td>\n      <td>-0.011187</td>\n    </tr>\n    <tr>\n      <th>Boardcomputer</th>\n      <td>0.601292</td>\n      <td>-0.719449</td>\n      <td>0.017715</td>\n      <td>0.720567</td>\n      <td>-0.353862</td>\n      <td>0.129715</td>\n      <td>0.089886</td>\n      <td>-0.037069</td>\n      <td>0.009312</td>\n      <td>0.089606</td>\n      <td>...</td>\n      <td>0.003891</td>\n      <td>0.030309</td>\n      <td>0.037010</td>\n      <td>-0.092482</td>\n      <td>0.166397</td>\n      <td>-0.107833</td>\n      <td>-0.038010</td>\n      <td>-0.034153</td>\n      <td>-0.043429</td>\n      <td>0.037349</td>\n    </tr>\n    <tr>\n      <th>CD_Player</th>\n      <td>0.481374</td>\n      <td>-0.510895</td>\n      <td>-0.016736</td>\n      <td>0.517008</td>\n      <td>-0.266826</td>\n      <td>0.102300</td>\n      <td>0.198220</td>\n      <td>-0.010967</td>\n      <td>0.057787</td>\n      <td>0.094653</td>\n      <td>...</td>\n      <td>-0.024205</td>\n      <td>0.040858</td>\n      <td>0.051325</td>\n      <td>-0.080013</td>\n      <td>0.087678</td>\n      <td>-0.075849</td>\n      <td>-0.004090</td>\n      <td>-0.027959</td>\n      <td>-0.055395</td>\n      <td>0.012694</td>\n    </tr>\n    <tr>\n      <th>Central_Lock</th>\n      <td>0.343458</td>\n      <td>-0.279631</td>\n      <td>0.010055</td>\n      <td>0.279490</td>\n      <td>-0.125177</td>\n      <td>0.250122</td>\n      <td>0.153307</td>\n      <td>-0.002502</td>\n      <td>0.072634</td>\n      <td>0.132092</td>\n      <td>...</td>\n      <td>0.008027</td>\n      <td>0.083948</td>\n      <td>-0.021863</td>\n      <td>-0.010257</td>\n      <td>0.042967</td>\n      <td>-0.079503</td>\n      <td>0.036587</td>\n      <td>-0.008576</td>\n      <td>-0.077499</td>\n      <td>-0.022875</td>\n    </tr>\n    <tr>\n      <th>Powered_Windows</th>\n      <td>0.356518</td>\n      <td>-0.283856</td>\n      <td>0.025185</td>\n      <td>0.280996</td>\n      <td>-0.156242</td>\n      <td>0.265593</td>\n      <td>0.145147</td>\n      <td>-0.005864</td>\n      <td>0.055299</td>\n      <td>0.107626</td>\n      <td>...</td>\n      <td>0.009655</td>\n      <td>0.073002</td>\n      <td>-0.000140</td>\n      <td>-0.021959</td>\n      <td>0.033948</td>\n      <td>-0.075416</td>\n      <td>0.047515</td>\n      <td>-0.006602</td>\n      <td>-0.090988</td>\n      <td>-0.021086</td>\n    </tr>\n    <tr>\n      <th>Power_Steering</th>\n      <td>0.064275</td>\n      <td>-0.069192</td>\n      <td>-0.055495</td>\n      <td>0.079676</td>\n      <td>0.007397</td>\n      <td>0.048850</td>\n      <td>0.086544</td>\n      <td>-0.004469</td>\n      <td>0.032933</td>\n      <td>0.059792</td>\n      <td>...</td>\n      <td>0.006908</td>\n      <td>-0.010333</td>\n      <td>0.039214</td>\n      <td>-0.014375</td>\n      <td>0.019792</td>\n      <td>-0.021553</td>\n      <td>-0.021681</td>\n      <td>-0.081536</td>\n      <td>0.022425</td>\n      <td>0.006908</td>\n    </tr>\n    <tr>\n      <th>Radio</th>\n      <td>-0.041887</td>\n      <td>0.013791</td>\n      <td>0.031601</td>\n      <td>-0.019607</td>\n      <td>0.013661</td>\n      <td>0.020998</td>\n      <td>0.072756</td>\n      <td>-0.014600</td>\n      <td>-0.000361</td>\n      <td>-0.008318</td>\n      <td>...</td>\n      <td>0.024227</td>\n      <td>-0.051836</td>\n      <td>-0.001911</td>\n      <td>0.031885</td>\n      <td>-0.058191</td>\n      <td>-0.018229</td>\n      <td>0.121285</td>\n      <td>0.015520</td>\n      <td>0.019888</td>\n      <td>-0.018937</td>\n    </tr>\n    <tr>\n      <th>Mistlamps</th>\n      <td>0.222083</td>\n      <td>-0.126895</td>\n      <td>-0.033504</td>\n      <td>0.133737</td>\n      <td>-0.074327</td>\n      <td>0.210571</td>\n      <td>0.023821</td>\n      <td>0.003077</td>\n      <td>0.017326</td>\n      <td>0.064705</td>\n      <td>...</td>\n      <td>0.007997</td>\n      <td>0.140416</td>\n      <td>0.001119</td>\n      <td>-0.006779</td>\n      <td>0.002560</td>\n      <td>-0.062261</td>\n      <td>-0.047725</td>\n      <td>-0.031081</td>\n      <td>-0.054455</td>\n      <td>0.007997</td>\n    </tr>\n    <tr>\n      <th>Sport_Model</th>\n      <td>0.164121</td>\n      <td>-0.110988</td>\n      <td>0.052789</td>\n      <td>0.102080</td>\n      <td>-0.044784</td>\n      <td>-0.006027</td>\n      <td>0.003779</td>\n      <td>0.013175</td>\n      <td>-0.035195</td>\n      <td>-0.129881</td>\n      <td>...</td>\n      <td>0.003314</td>\n      <td>0.088026</td>\n      <td>0.030789</td>\n      <td>-0.084499</td>\n      <td>0.043519</td>\n      <td>-0.024760</td>\n      <td>-0.052408</td>\n      <td>-0.034611</td>\n      <td>-0.013636</td>\n      <td>-0.029964</td>\n    </tr>\n    <tr>\n      <th>Backseat_Divider</th>\n      <td>0.102569</td>\n      <td>-0.116751</td>\n      <td>0.023245</td>\n      <td>0.113237</td>\n      <td>-0.045658</td>\n      <td>0.010908</td>\n      <td>0.037741</td>\n      <td>-0.018876</td>\n      <td>-0.055711</td>\n      <td>-0.022542</td>\n      <td>...</td>\n      <td>-0.011259</td>\n      <td>0.067718</td>\n      <td>0.029273</td>\n      <td>-0.020418</td>\n      <td>0.086097</td>\n      <td>-0.021425</td>\n      <td>-0.154138</td>\n      <td>-0.096756</td>\n      <td>-0.044146</td>\n      <td>0.024993</td>\n    </tr>\n    <tr>\n      <th>Metallic_Rim</th>\n      <td>0.108564</td>\n      <td>-0.040045</td>\n      <td>0.023506</td>\n      <td>0.036022</td>\n      <td>-0.013599</td>\n      <td>0.206784</td>\n      <td>0.053829</td>\n      <td>-0.078095</td>\n      <td>0.003236</td>\n      <td>-0.039555</td>\n      <td>...</td>\n      <td>0.052380</td>\n      <td>0.146851</td>\n      <td>0.008937</td>\n      <td>-0.038532</td>\n      <td>-0.062012</td>\n      <td>-0.034578</td>\n      <td>-0.024621</td>\n      <td>0.005929</td>\n      <td>0.007757</td>\n      <td>0.052380</td>\n    </tr>\n    <tr>\n      <th>Radio_cassette</th>\n      <td>-0.043179</td>\n      <td>0.012857</td>\n      <td>0.032576</td>\n      <td>-0.018844</td>\n      <td>0.015770</td>\n      <td>0.019919</td>\n      <td>0.071530</td>\n      <td>-0.014150</td>\n      <td>-0.000470</td>\n      <td>-0.008265</td>\n      <td>...</td>\n      <td>0.024365</td>\n      <td>-0.051166</td>\n      <td>-0.005901</td>\n      <td>0.027306</td>\n      <td>-0.057289</td>\n      <td>-0.012300</td>\n      <td>0.122127</td>\n      <td>0.015655</td>\n      <td>0.020220</td>\n      <td>-0.018884</td>\n    </tr>\n    <tr>\n      <th>Parking_Assistant</th>\n      <td>0.044375</td>\n      <td>-0.048172</td>\n      <td>0.007113</td>\n      <td>0.047171</td>\n      <td>-0.064318</td>\n      <td>0.029990</td>\n      <td>0.036691</td>\n      <td>0.159985</td>\n      <td>0.002883</td>\n      <td>0.025893</td>\n      <td>...</td>\n      <td>-0.002418</td>\n      <td>-0.020701</td>\n      <td>0.007032</td>\n      <td>0.014203</td>\n      <td>-0.027217</td>\n      <td>0.007545</td>\n      <td>0.031285</td>\n      <td>-0.002793</td>\n      <td>-0.007851</td>\n      <td>-0.002418</td>\n    </tr>\n    <tr>\n      <th>Tow_Bar</th>\n      <td>-0.172369</td>\n      <td>0.188720</td>\n      <td>-0.042170</td>\n      <td>-0.182206</td>\n      <td>0.084153</td>\n      <td>0.068271</td>\n      <td>0.148536</td>\n      <td>0.018786</td>\n      <td>0.002725</td>\n      <td>0.102292</td>\n      <td>...</td>\n      <td>0.039715</td>\n      <td>-0.036946</td>\n      <td>0.009251</td>\n      <td>0.012396</td>\n      <td>-0.074992</td>\n      <td>0.081668</td>\n      <td>0.022870</td>\n      <td>-0.003287</td>\n      <td>-0.027957</td>\n      <td>-0.028381</td>\n    </tr>\n    <tr>\n      <th>Fuel_Type_CNG</th>\n      <td>-0.039536</td>\n      <td>0.002389</td>\n      <td>0.001289</td>\n      <td>-0.002637</td>\n      <td>0.144016</td>\n      <td>0.062109</td>\n      <td>0.021009</td>\n      <td>0.001486</td>\n      <td>0.005941</td>\n      <td>0.009680</td>\n      <td>...</td>\n      <td>-0.005008</td>\n      <td>0.032969</td>\n      <td>-0.021855</td>\n      <td>-0.010805</td>\n      <td>-0.040548</td>\n      <td>0.044143</td>\n      <td>-0.010260</td>\n      <td>-0.005785</td>\n      <td>0.028043</td>\n      <td>-0.005008</td>\n    </tr>\n    <tr>\n      <th>Fuel_Type_Diesel</th>\n      <td>0.054084</td>\n      <td>-0.097740</td>\n      <td>0.051501</td>\n      <td>0.088986</td>\n      <td>0.403060</td>\n      <td>-0.533453</td>\n      <td>-0.012420</td>\n      <td>-0.084490</td>\n      <td>0.327723</td>\n      <td>0.025495</td>\n      <td>...</td>\n      <td>-0.015916</td>\n      <td>-0.010682</td>\n      <td>-0.020009</td>\n      <td>-0.035805</td>\n      <td>0.024869</td>\n      <td>-0.011399</td>\n      <td>0.014741</td>\n      <td>-0.018384</td>\n      <td>0.118189</td>\n      <td>-0.015916</td>\n    </tr>\n    <tr>\n      <th>Fuel_Type_Petrol</th>\n      <td>-0.038516</td>\n      <td>0.092611</td>\n      <td>-0.049646</td>\n      <td>-0.084162</td>\n      <td>-0.433160</td>\n      <td>0.489110</td>\n      <td>0.004872</td>\n      <td>0.080249</td>\n      <td>-0.315170</td>\n      <td>-0.027589</td>\n      <td>...</td>\n      <td>0.016878</td>\n      <td>-0.000774</td>\n      <td>0.026402</td>\n      <td>0.037816</td>\n      <td>-0.010259</td>\n      <td>-0.003810</td>\n      <td>-0.010670</td>\n      <td>0.019496</td>\n      <td>-0.122290</td>\n      <td>0.016878</td>\n    </tr>\n    <tr>\n      <th>Color_Beige</th>\n      <td>-0.022684</td>\n      <td>0.023098</td>\n      <td>0.028902</td>\n      <td>-0.028480</td>\n      <td>-0.006720</td>\n      <td>0.025963</td>\n      <td>0.031764</td>\n      <td>-0.011114</td>\n      <td>0.002496</td>\n      <td>-0.033636</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>-0.017921</td>\n      <td>-0.022668</td>\n      <td>-0.019462</td>\n      <td>-0.023563</td>\n      <td>-0.022418</td>\n      <td>-0.013942</td>\n      <td>-0.002418</td>\n      <td>-0.006796</td>\n      <td>-0.002094</td>\n    </tr>\n    <tr>\n      <th>Color_Black</th>\n      <td>0.034896</td>\n      <td>-0.019399</td>\n      <td>0.008067</td>\n      <td>0.018052</td>\n      <td>0.039012</td>\n      <td>-0.002040</td>\n      <td>0.061789</td>\n      <td>-0.041491</td>\n      <td>-0.007964</td>\n      <td>-0.099880</td>\n      <td>...</td>\n      <td>-0.017921</td>\n      <td>1.000000</td>\n      <td>-0.194049</td>\n      <td>-0.166601</td>\n      <td>-0.201705</td>\n      <td>-0.191911</td>\n      <td>-0.119348</td>\n      <td>-0.020701</td>\n      <td>-0.058180</td>\n      <td>-0.017921</td>\n    </tr>\n    <tr>\n      <th>Color_Blue</th>\n      <td>0.014431</td>\n      <td>-0.032955</td>\n      <td>0.057279</td>\n      <td>0.022763</td>\n      <td>-0.003840</td>\n      <td>-0.027949</td>\n      <td>0.044972</td>\n      <td>-0.005847</td>\n      <td>0.029421</td>\n      <td>-0.044962</td>\n      <td>...</td>\n      <td>-0.022668</td>\n      <td>-0.194049</td>\n      <td>1.000000</td>\n      <td>-0.210728</td>\n      <td>-0.255131</td>\n      <td>-0.242743</td>\n      <td>-0.150960</td>\n      <td>-0.026184</td>\n      <td>-0.073590</td>\n      <td>-0.022668</td>\n    </tr>\n    <tr>\n      <th>Color_Green</th>\n      <td>-0.104963</td>\n      <td>0.103530</td>\n      <td>-0.030403</td>\n      <td>-0.098638</td>\n      <td>-0.017439</td>\n      <td>0.021242</td>\n      <td>0.084797</td>\n      <td>0.056846</td>\n      <td>-0.010579</td>\n      <td>0.060186</td>\n      <td>...</td>\n      <td>-0.019462</td>\n      <td>-0.166601</td>\n      <td>-0.210728</td>\n      <td>1.000000</td>\n      <td>-0.219043</td>\n      <td>-0.208407</td>\n      <td>-0.129607</td>\n      <td>-0.022480</td>\n      <td>-0.063181</td>\n      <td>-0.019462</td>\n    </tr>\n    <tr>\n      <th>Color_Grey</th>\n      <td>0.169947</td>\n      <td>-0.137660</td>\n      <td>-0.031214</td>\n      <td>0.144152</td>\n      <td>-0.112194</td>\n      <td>0.021349</td>\n      <td>0.072634</td>\n      <td>0.001725</td>\n      <td>-0.009867</td>\n      <td>0.059174</td>\n      <td>...</td>\n      <td>-0.023563</td>\n      <td>-0.201705</td>\n      <td>-0.255131</td>\n      <td>-0.219043</td>\n      <td>1.000000</td>\n      <td>-0.252321</td>\n      <td>-0.156916</td>\n      <td>-0.027217</td>\n      <td>-0.076494</td>\n      <td>-0.023563</td>\n    </tr>\n    <tr>\n      <th>Color_Red</th>\n      <td>-0.103803</td>\n      <td>0.098365</td>\n      <td>0.008646</td>\n      <td>-0.100526</td>\n      <td>0.052190</td>\n      <td>0.011700</td>\n      <td>-0.261828</td>\n      <td>-0.026799</td>\n      <td>-0.008865</td>\n      <td>-0.000541</td>\n      <td>...</td>\n      <td>-0.022418</td>\n      <td>-0.191911</td>\n      <td>-0.242743</td>\n      <td>-0.208407</td>\n      <td>-0.252321</td>\n      <td>1.000000</td>\n      <td>-0.149297</td>\n      <td>-0.025896</td>\n      <td>-0.072780</td>\n      <td>-0.022418</td>\n    </tr>\n    <tr>\n      <th>Color_Silver</th>\n      <td>0.028562</td>\n      <td>-0.028550</td>\n      <td>-0.017838</td>\n      <td>0.031958</td>\n      <td>0.006113</td>\n      <td>0.014300</td>\n      <td>0.104902</td>\n      <td>0.045772</td>\n      <td>-0.001351</td>\n      <td>0.028644</td>\n      <td>...</td>\n      <td>-0.013942</td>\n      <td>-0.119348</td>\n      <td>-0.150960</td>\n      <td>-0.129607</td>\n      <td>-0.156916</td>\n      <td>-0.149297</td>\n      <td>1.000000</td>\n      <td>-0.016104</td>\n      <td>-0.045261</td>\n      <td>-0.013942</td>\n    </tr>\n    <tr>\n      <th>Color_Violet</th>\n      <td>-0.016848</td>\n      <td>0.017916</td>\n      <td>-0.028353</td>\n      <td>-0.012880</td>\n      <td>0.018810</td>\n      <td>-0.012359</td>\n      <td>0.036691</td>\n      <td>-0.012837</td>\n      <td>-0.015804</td>\n      <td>0.012019</td>\n      <td>...</td>\n      <td>-0.002418</td>\n      <td>-0.020701</td>\n      <td>-0.026184</td>\n      <td>-0.022480</td>\n      <td>-0.027217</td>\n      <td>-0.025896</td>\n      <td>-0.016104</td>\n      <td>1.000000</td>\n      <td>-0.007851</td>\n      <td>-0.002418</td>\n    </tr>\n    <tr>\n      <th>Color_White</th>\n      <td>-0.103360</td>\n      <td>0.050933</td>\n      <td>-0.001445</td>\n      <td>-0.050977</td>\n      <td>0.130403</td>\n      <td>-0.093928</td>\n      <td>-0.203738</td>\n      <td>-0.036079</td>\n      <td>0.027304</td>\n      <td>-0.010245</td>\n      <td>...</td>\n      <td>-0.006796</td>\n      <td>-0.058180</td>\n      <td>-0.073590</td>\n      <td>-0.063181</td>\n      <td>-0.076494</td>\n      <td>-0.072780</td>\n      <td>-0.045261</td>\n      <td>-0.007851</td>\n      <td>1.000000</td>\n      <td>-0.006796</td>\n    </tr>\n    <tr>\n      <th>Color_Yellow</th>\n      <td>0.022726</td>\n      <td>-0.040883</td>\n      <td>0.001609</td>\n      <td>0.040837</td>\n      <td>-0.037995</td>\n      <td>-0.000516</td>\n      <td>0.031764</td>\n      <td>-0.011114</td>\n      <td>-0.011884</td>\n      <td>0.014409</td>\n      <td>...</td>\n      <td>-0.002094</td>\n      <td>-0.017921</td>\n      <td>-0.022668</td>\n      <td>-0.019462</td>\n      <td>-0.023563</td>\n      <td>-0.022418</td>\n      <td>-0.013942</td>\n      <td>-0.002418</td>\n      <td>-0.006796</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>48 rows × 48 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-01-10T05:01:34.048292Z","iopub.execute_input":"2024-01-10T05:01:34.049175Z","iopub.status.idle":"2024-01-10T05:01:34.184755Z","shell.execute_reply.started":"2024-01-10T05:01:34.049137Z","shell.execute_reply":"2024-01-10T05:01:34.183546Z"},"trusted":true},"execution_count":175,"outputs":[{"execution_count":175,"output_type":"execute_result","data":{"text/plain":"              Price    Age_08_04    Mfg_Month     Mfg_Year             KM  \\\ncount   1436.000000  1436.000000  1436.000000  1436.000000    1436.000000   \nmean   10730.824513    55.947075     5.548747  1999.625348   68533.259749   \nstd     3626.964585    18.599988     3.354085     1.540722   37506.448872   \nmin     4350.000000     1.000000     1.000000  1998.000000       1.000000   \n25%     8450.000000    44.000000     3.000000  1998.000000   43000.000000   \n50%     9900.000000    61.000000     5.000000  1999.000000   63389.500000   \n75%    11950.000000    70.000000     8.000000  2001.000000   87020.750000   \nmax    32500.000000    80.000000    12.000000  2004.000000  243000.000000   \n\n                HP    Met_Color    Automatic           CC        Doors  ...  \\\ncount  1436.000000  1436.000000  1436.000000   1436.00000  1436.000000  ...   \nmean    101.502089     0.674791     0.055710   1576.85585     4.033426  ...   \nstd      14.981080     0.468616     0.229441    424.38677     0.952677  ...   \nmin      69.000000     0.000000     0.000000   1300.00000     2.000000  ...   \n25%      90.000000     0.000000     0.000000   1400.00000     3.000000  ...   \n50%     110.000000     1.000000     0.000000   1600.00000     4.000000  ...   \n75%     110.000000     1.000000     0.000000   1600.00000     5.000000  ...   \nmax     192.000000     1.000000     1.000000  16000.00000     5.000000  ...   \n\n       Color_Beige  Color_Black   Color_Blue  Color_Green   Color_Grey  \\\ncount  1436.000000  1436.000000  1436.000000  1436.000000  1436.000000   \nmean      0.002089     0.133008     0.197075     0.153203     0.209610   \nstd       0.045675     0.339702     0.397928     0.360309     0.407172   \nmin       0.000000     0.000000     0.000000     0.000000     0.000000   \n25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n75%       0.000000     0.000000     0.000000     0.000000     0.000000   \nmax       1.000000     1.000000     1.000000     1.000000     1.000000   \n\n         Color_Red  Color_Silver  Color_Violet  Color_White  Color_Yellow  \ncount  1436.000000   1436.000000   1436.000000  1436.000000   1436.000000  \nmean      0.193593      0.084958      0.002786     0.021588      0.002089  \nstd       0.395251      0.278917      0.052723     0.145384      0.045675  \nmin       0.000000      0.000000      0.000000     0.000000      0.000000  \n25%       0.000000      0.000000      0.000000     0.000000      0.000000  \n50%       0.000000      0.000000      0.000000     0.000000      0.000000  \n75%       0.000000      0.000000      0.000000     0.000000      0.000000  \nmax       1.000000      1.000000      1.000000     1.000000      1.000000  \n\n[8 rows x 48 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Price</th>\n      <th>Age_08_04</th>\n      <th>Mfg_Month</th>\n      <th>Mfg_Year</th>\n      <th>KM</th>\n      <th>HP</th>\n      <th>Met_Color</th>\n      <th>Automatic</th>\n      <th>CC</th>\n      <th>Doors</th>\n      <th>...</th>\n      <th>Color_Beige</th>\n      <th>Color_Black</th>\n      <th>Color_Blue</th>\n      <th>Color_Green</th>\n      <th>Color_Grey</th>\n      <th>Color_Red</th>\n      <th>Color_Silver</th>\n      <th>Color_Violet</th>\n      <th>Color_White</th>\n      <th>Color_Yellow</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1436.000000</td>\n      <td>1436.000000</td>\n      <td>1436.000000</td>\n      <td>1436.000000</td>\n      <td>1436.000000</td>\n      <td>1436.000000</td>\n      <td>1436.000000</td>\n      <td>1436.000000</td>\n      <td>1436.00000</td>\n      <td>1436.000000</td>\n      <td>...</td>\n      <td>1436.000000</td>\n      <td>1436.000000</td>\n      <td>1436.000000</td>\n      <td>1436.000000</td>\n      <td>1436.000000</td>\n      <td>1436.000000</td>\n      <td>1436.000000</td>\n      <td>1436.000000</td>\n      <td>1436.000000</td>\n      <td>1436.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>10730.824513</td>\n      <td>55.947075</td>\n      <td>5.548747</td>\n      <td>1999.625348</td>\n      <td>68533.259749</td>\n      <td>101.502089</td>\n      <td>0.674791</td>\n      <td>0.055710</td>\n      <td>1576.85585</td>\n      <td>4.033426</td>\n      <td>...</td>\n      <td>0.002089</td>\n      <td>0.133008</td>\n      <td>0.197075</td>\n      <td>0.153203</td>\n      <td>0.209610</td>\n      <td>0.193593</td>\n      <td>0.084958</td>\n      <td>0.002786</td>\n      <td>0.021588</td>\n      <td>0.002089</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>3626.964585</td>\n      <td>18.599988</td>\n      <td>3.354085</td>\n      <td>1.540722</td>\n      <td>37506.448872</td>\n      <td>14.981080</td>\n      <td>0.468616</td>\n      <td>0.229441</td>\n      <td>424.38677</td>\n      <td>0.952677</td>\n      <td>...</td>\n      <td>0.045675</td>\n      <td>0.339702</td>\n      <td>0.397928</td>\n      <td>0.360309</td>\n      <td>0.407172</td>\n      <td>0.395251</td>\n      <td>0.278917</td>\n      <td>0.052723</td>\n      <td>0.145384</td>\n      <td>0.045675</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>4350.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1998.000000</td>\n      <td>1.000000</td>\n      <td>69.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1300.00000</td>\n      <td>2.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>8450.000000</td>\n      <td>44.000000</td>\n      <td>3.000000</td>\n      <td>1998.000000</td>\n      <td>43000.000000</td>\n      <td>90.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1400.00000</td>\n      <td>3.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>9900.000000</td>\n      <td>61.000000</td>\n      <td>5.000000</td>\n      <td>1999.000000</td>\n      <td>63389.500000</td>\n      <td>110.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1600.00000</td>\n      <td>4.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>11950.000000</td>\n      <td>70.000000</td>\n      <td>8.000000</td>\n      <td>2001.000000</td>\n      <td>87020.750000</td>\n      <td>110.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1600.00000</td>\n      <td>5.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>32500.000000</td>\n      <td>80.000000</td>\n      <td>12.000000</td>\n      <td>2004.000000</td>\n      <td>243000.000000</td>\n      <td>192.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>16000.00000</td>\n      <td>5.000000</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 48 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"x=df.drop('Price',axis=1)\ny=df['Price']","metadata":{"execution":{"iopub.status.busy":"2024-01-10T05:01:34.186066Z","iopub.execute_input":"2024-01-10T05:01:34.186451Z","iopub.status.idle":"2024-01-10T05:01:34.193251Z","shell.execute_reply.started":"2024-01-10T05:01:34.186416Z","shell.execute_reply":"2024-01-10T05:01:34.191776Z"},"trusted":true},"execution_count":176,"outputs":[]},{"cell_type":"code","source":"y.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-10T05:01:34.194794Z","iopub.execute_input":"2024-01-10T05:01:34.195217Z","iopub.status.idle":"2024-01-10T05:01:34.208264Z","shell.execute_reply.started":"2024-01-10T05:01:34.195182Z","shell.execute_reply":"2024-01-10T05:01:34.207057Z"},"trusted":true},"execution_count":177,"outputs":[{"execution_count":177,"output_type":"execute_result","data":{"text/plain":"0    13500\n1    13750\n2    13950\n3    14950\n4    13750\nName: Price, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"x.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-10T05:01:34.210135Z","iopub.execute_input":"2024-01-10T05:01:34.210928Z","iopub.status.idle":"2024-01-10T05:01:34.241338Z","shell.execute_reply.started":"2024-01-10T05:01:34.210882Z","shell.execute_reply":"2024-01-10T05:01:34.240263Z"},"trusted":true},"execution_count":178,"outputs":[{"execution_count":178,"output_type":"execute_result","data":{"text/plain":"   Age_08_04  Mfg_Month  Mfg_Year     KM  HP  Met_Color  Automatic    CC  \\\n0         23         10      2002  46986  90          1          0  2000   \n1         23         10      2002  72937  90          1          0  2000   \n2         24          9      2002  41711  90          1          0  2000   \n3         26          7      2002  48000  90          0          0  2000   \n4         30          3      2002  38500  90          0          0  2000   \n\n   Doors  Cylinders  ...  Color_Beige  Color_Black  Color_Blue  Color_Green  \\\n0      3          4  ...          0.0          0.0         1.0          0.0   \n1      3          4  ...          0.0          0.0         0.0          0.0   \n2      3          4  ...          0.0          0.0         1.0          0.0   \n3      3          4  ...          0.0          1.0         0.0          0.0   \n4      3          4  ...          0.0          1.0         0.0          0.0   \n\n   Color_Grey  Color_Red  Color_Silver  Color_Violet  Color_White  \\\n0         0.0        0.0           0.0           0.0          0.0   \n1         0.0        0.0           1.0           0.0          0.0   \n2         0.0        0.0           0.0           0.0          0.0   \n3         0.0        0.0           0.0           0.0          0.0   \n4         0.0        0.0           0.0           0.0          0.0   \n\n   Color_Yellow  \n0           0.0  \n1           0.0  \n2           0.0  \n3           0.0  \n4           0.0  \n\n[5 rows x 47 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age_08_04</th>\n      <th>Mfg_Month</th>\n      <th>Mfg_Year</th>\n      <th>KM</th>\n      <th>HP</th>\n      <th>Met_Color</th>\n      <th>Automatic</th>\n      <th>CC</th>\n      <th>Doors</th>\n      <th>Cylinders</th>\n      <th>...</th>\n      <th>Color_Beige</th>\n      <th>Color_Black</th>\n      <th>Color_Blue</th>\n      <th>Color_Green</th>\n      <th>Color_Grey</th>\n      <th>Color_Red</th>\n      <th>Color_Silver</th>\n      <th>Color_Violet</th>\n      <th>Color_White</th>\n      <th>Color_Yellow</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>23</td>\n      <td>10</td>\n      <td>2002</td>\n      <td>46986</td>\n      <td>90</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2000</td>\n      <td>3</td>\n      <td>4</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>23</td>\n      <td>10</td>\n      <td>2002</td>\n      <td>72937</td>\n      <td>90</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2000</td>\n      <td>3</td>\n      <td>4</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>24</td>\n      <td>9</td>\n      <td>2002</td>\n      <td>41711</td>\n      <td>90</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2000</td>\n      <td>3</td>\n      <td>4</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>26</td>\n      <td>7</td>\n      <td>2002</td>\n      <td>48000</td>\n      <td>90</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2000</td>\n      <td>3</td>\n      <td>4</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>30</td>\n      <td>3</td>\n      <td>2002</td>\n      <td>38500</td>\n      <td>90</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2000</td>\n      <td>3</td>\n      <td>4</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 47 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.20,random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T05:01:34.242599Z","iopub.execute_input":"2024-01-10T05:01:34.242919Z","iopub.status.idle":"2024-01-10T05:01:34.252687Z","shell.execute_reply.started":"2024-01-10T05:01:34.242891Z","shell.execute_reply":"2024-01-10T05:01:34.251410Z"},"trusted":true},"execution_count":179,"outputs":[]},{"cell_type":"code","source":"print(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T05:01:34.254192Z","iopub.execute_input":"2024-01-10T05:01:34.254685Z","iopub.status.idle":"2024-01-10T05:01:34.262150Z","shell.execute_reply.started":"2024-01-10T05:01:34.254642Z","shell.execute_reply":"2024-01-10T05:01:34.260845Z"},"trusted":true},"execution_count":180,"outputs":[{"name":"stdout","text":"(1148, 47)\n(288, 47)\n(1148,)\n(288,)\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import r2_score","metadata":{"execution":{"iopub.status.busy":"2024-01-10T05:01:34.266109Z","iopub.execute_input":"2024-01-10T05:01:34.267045Z","iopub.status.idle":"2024-01-10T05:01:34.271709Z","shell.execute_reply.started":"2024-01-10T05:01:34.267006Z","shell.execute_reply":"2024-01-10T05:01:34.270682Z"},"trusted":true},"execution_count":181,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr=LinearRegression().fit(x_train,y_train)\nypred1=lr.predict(x_test)\nr2_score(y_test,ypred1)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T05:01:34.272979Z","iopub.execute_input":"2024-01-10T05:01:34.273325Z","iopub.status.idle":"2024-01-10T05:01:34.310880Z","shell.execute_reply.started":"2024-01-10T05:01:34.273294Z","shell.execute_reply":"2024-01-10T05:01:34.309315Z"},"trusted":true},"execution_count":182,"outputs":[{"execution_count":182,"output_type":"execute_result","data":{"text/plain":"0.8865168289210364"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nrfr=RandomForestRegressor().fit(x_train,y_train)\nypred2=rfr.predict(x_test)\nr2_score(y_test,ypred2)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T05:01:34.313443Z","iopub.execute_input":"2024-01-10T05:01:34.317045Z","iopub.status.idle":"2024-01-10T05:01:35.287429Z","shell.execute_reply.started":"2024-01-10T05:01:34.316968Z","shell.execute_reply":"2024-01-10T05:01:35.286412Z"},"trusted":true},"execution_count":183,"outputs":[{"execution_count":183,"output_type":"execute_result","data":{"text/plain":"0.9300366136857071"},"metadata":{}}]},{"cell_type":"code","source":"from xgboost import XGBRegressor\nxgb=XGBRegressor().fit(x_train,y_train)\nypred3=xgb.predict(x_test)\nr2_score(y_test,ypred3)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T05:01:35.288865Z","iopub.execute_input":"2024-01-10T05:01:35.289194Z","iopub.status.idle":"2024-01-10T05:01:35.469370Z","shell.execute_reply.started":"2024-01-10T05:01:35.289163Z","shell.execute_reply":"2024-01-10T05:01:35.468336Z"},"trusted":true},"execution_count":184,"outputs":[{"execution_count":184,"output_type":"execute_result","data":{"text/plain":"0.9291763515864222"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\ngbr=GradientBoostingRegressor(loss='squared_error').fit(x_train,y_train)\nypred4=gbr.predict(x_test)\nr2_score(y_test,ypred4)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T05:01:35.474485Z","iopub.execute_input":"2024-01-10T05:01:35.475281Z","iopub.status.idle":"2024-01-10T05:01:35.776170Z","shell.execute_reply.started":"2024-01-10T05:01:35.475235Z","shell.execute_reply":"2024-01-10T05:01:35.775064Z"},"trusted":true},"execution_count":185,"outputs":[{"execution_count":185,"output_type":"execute_result","data":{"text/plain":"0.9362946204976581"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparameters= [{'loss':['squared_error','absolute_error', 'huber'], \n            'learning_rate' :[0.1,0.2,0.4,0.6,0.8,0.9],\n              'n_estimators':[100,200,500],\n              'criterion' :['friedman_mse', 'squared_error']}]\n              \n              ","metadata":{"execution":{"iopub.status.busy":"2024-01-10T05:01:35.777592Z","iopub.execute_input":"2024-01-10T05:01:35.777956Z","iopub.status.idle":"2024-01-10T05:01:35.788292Z","shell.execute_reply.started":"2024-01-10T05:01:35.777916Z","shell.execute_reply":"2024-01-10T05:01:35.786219Z"},"trusted":true},"execution_count":186,"outputs":[]},{"cell_type":"code","source":"gbr=GradientBoostingRegressor()\n\ngbr_grid= GridSearchCV(estimator=gbr, param_grid = parameters, cv = 5,verbose=2, n_jobs = -1)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T05:01:35.790478Z","iopub.execute_input":"2024-01-10T05:01:35.790962Z","iopub.status.idle":"2024-01-10T05:01:35.797198Z","shell.execute_reply.started":"2024-01-10T05:01:35.790923Z","shell.execute_reply":"2024-01-10T05:01:35.795520Z"},"trusted":true},"execution_count":187,"outputs":[]},{"cell_type":"code","source":"gbr_grid.fit(x_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T05:01:35.798708Z","iopub.execute_input":"2024-01-10T05:01:35.800217Z","iopub.status.idle":"2024-01-10T05:05:14.258991Z","shell.execute_reply.started":"2024-01-10T05:01:35.800167Z","shell.execute_reply":"2024-01-10T05:05:14.257473Z"},"trusted":true},"execution_count":188,"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 108 candidates, totalling 540 fits\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=squared_error, n_estimators=100; total time=   0.4s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=squared_error, n_estimators=100; total time=   0.3s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=squared_error, n_estimators=500; total time=   1.9s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=absolute_error, n_estimators=100; total time=   0.8s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=absolute_error, n_estimators=100; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=absolute_error, n_estimators=200; total time=   1.3s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=absolute_error, n_estimators=500; total time=   3.2s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=huber, n_estimators=100; total time=   0.9s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=huber, n_estimators=100; total time=   0.9s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=huber, n_estimators=200; total time=   1.8s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=huber, n_estimators=200; total time=   1.7s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=huber, n_estimators=500; total time=   4.3s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=squared_error, n_estimators=500; total time=   1.7s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=absolute_error, n_estimators=100; total time=   0.8s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=absolute_error, n_estimators=100; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=absolute_error, n_estimators=200; total time=   1.8s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=absolute_error, n_estimators=500; total time=   3.0s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=huber, n_estimators=100; total time=   0.9s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=huber, n_estimators=100; total time=   0.8s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=huber, n_estimators=200; total time=   1.7s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=huber, n_estimators=200; total time=   1.7s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=huber, n_estimators=500; total time=   4.1s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=squared_error, n_estimators=500; total time=   1.7s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=absolute_error, n_estimators=100; total time=   0.6s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=absolute_error, n_estimators=100; total time=   0.6s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=absolute_error, n_estimators=200; total time=   1.2s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=absolute_error, n_estimators=500; total time=   2.7s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=absolute_error, n_estimators=500; total time=   2.8s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=huber, n_estimators=200; total time=   1.6s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=huber, n_estimators=500; total time=   4.1s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=squared_error, n_estimators=100; total time=   0.3s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=squared_error, n_estimators=100; total time=   0.3s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=squared_error, n_estimators=500; total time=   2.0s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=squared_error, n_estimators=500; total time=   1.7s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=absolute_error, n_estimators=100; total time=   0.6s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=absolute_error, n_estimators=200; total time=   1.2s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=absolute_error, n_estimators=500; total time=   3.0s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=huber, n_estimators=100; total time=   0.8s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=huber, n_estimators=100; total time=   0.8s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=huber, n_estimators=200; total time=   1.6s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=huber, n_estimators=500; total time=   4.1s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=squared_error, n_estimators=100; total time=   0.3s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=squared_error, n_estimators=100; total time=   0.4s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=squared_error, n_estimators=100; total time=   0.3s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=squared_error, n_estimators=100; total time=   0.3s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=squared_error, n_estimators=500; total time=   1.6s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=absolute_error, n_estimators=100; total time=   0.6s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=absolute_error, n_estimators=100; total time=   0.6s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=absolute_error, n_estimators=200; total time=   1.2s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=absolute_error, n_estimators=500; total time=   2.9s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=absolute_error, n_estimators=500; total time=   3.0s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=huber, n_estimators=200; total time=   1.6s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=huber, n_estimators=500; total time=   4.5s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=squared_error, n_estimators=100; total time=   0.3s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=squared_error, n_estimators=500; total time=   1.6s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=absolute_error, n_estimators=100; total time=   0.6s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=absolute_error, n_estimators=100; total time=   0.6s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=absolute_error, n_estimators=100; total time=   0.6s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=absolute_error, n_estimators=200; total time=   1.2s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=absolute_error, n_estimators=500; total time=   3.0s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=huber, n_estimators=100; total time=   0.8s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=huber, n_estimators=100; total time=   0.8s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=huber, n_estimators=200; total time=   1.6s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=huber, n_estimators=500; total time=   3.9s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=squared_error, n_estimators=100; total time=   0.3s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=squared_error, n_estimators=100; total time=   0.3s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=squared_error, n_estimators=100; total time=   0.3s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=squared_error, n_estimators=500; total time=   1.9s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=squared_error, n_estimators=500; total time=   1.9s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=absolute_error, n_estimators=200; total time=   1.4s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=absolute_error, n_estimators=500; total time=   3.1s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=huber, n_estimators=100; total time=   0.9s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=huber, n_estimators=100; total time=   0.9s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=huber, n_estimators=200; total time=   1.8s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=huber, n_estimators=500; total time=   4.4s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=huber, n_estimators=500; total time=   4.2s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=squared_error, n_estimators=500; total time=   1.8s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=absolute_error, n_estimators=200; total time=   1.7s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=absolute_error, n_estimators=200; total time=   1.3s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=absolute_error, n_estimators=500; total time=   3.0s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=huber, n_estimators=100; total time=   0.8s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=huber, n_estimators=200; total time=   1.7s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=huber, n_estimators=500; total time=   4.3s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=squared_error, n_estimators=100; total time=   0.3s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=squared_error, n_estimators=100; total time=   0.3s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=squared_error, n_estimators=100; total time=   0.3s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=squared_error, n_estimators=100; total time=   0.3s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=squared_error, n_estimators=500; total time=   1.7s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=squared_error, n_estimators=500; total time=   1.7s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=absolute_error, n_estimators=100; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=absolute_error, n_estimators=200; total time=   1.2s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=absolute_error, n_estimators=500; total time=   2.8s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=huber, n_estimators=100; total time=   0.8s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=huber, n_estimators=100; total time=   0.8s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=huber, n_estimators=200; total time=   1.6s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=huber, n_estimators=500; total time=   4.1s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=squared_error, n_estimators=100; total time=   0.3s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=squared_error, n_estimators=100; total time=   0.3s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=squared_error, n_estimators=100; total time=   0.3s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=squared_error, n_estimators=200; total time=   0.9s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=squared_error, n_estimators=500; total time=   1.8s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=squared_error, n_estimators=500; total time=   1.7s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=absolute_error, n_estimators=200; total time=   1.2s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=absolute_error, n_estimators=500; total time=   3.1s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=absolute_error, n_estimators=500; total time=   3.0s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=huber, n_estimators=200; total time=   1.7s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=huber, n_estimators=500; total time=   4.1s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=squared_error, n_estimators=100; total time=   0.3s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=squared_error, n_estimators=500; total time=   1.6s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=absolute_error, n_estimators=100; total time=   0.6s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=absolute_error, n_estimators=100; total time=   0.6s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=absolute_error, n_estimators=200; total time=   1.2s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=absolute_error, n_estimators=500; total time=   2.9s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=huber, n_estimators=100; total time=   0.9s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=huber, n_estimators=100; total time=   0.8s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=huber, n_estimators=200; total time=   1.6s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=huber, n_estimators=200; total time=   1.6s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=huber, n_estimators=500; total time=   4.2s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=squared_error, n_estimators=100; total time=   0.3s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=squared_error, n_estimators=500; total time=   1.6s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=absolute_error, n_estimators=100; total time=   0.6s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=absolute_error, n_estimators=100; total time=   0.6s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=absolute_error, n_estimators=200; total time=   1.2s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=absolute_error, n_estimators=500; total time=   3.0s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=absolute_error, n_estimators=500; total time=   3.0s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=huber, n_estimators=200; total time=   1.6s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=huber, n_estimators=500; total time=   4.0s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=squared_error, n_estimators=100; total time=   0.3s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=squared_error, n_estimators=500; total time=   1.6s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=absolute_error, n_estimators=100; total time=   0.7s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=absolute_error, n_estimators=100; total time=   0.7s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=absolute_error, n_estimators=200; total time=   1.3s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=absolute_error, n_estimators=500; total time=   2.9s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=squared_error, n_estimators=100; total time=   0.4s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=squared_error, n_estimators=500; total time=   1.9s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=absolute_error, n_estimators=100; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=absolute_error, n_estimators=200; total time=   1.4s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=absolute_error, n_estimators=500; total time=   3.3s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=absolute_error, n_estimators=500; total time=   3.1s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=huber, n_estimators=200; total time=   1.7s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=huber, n_estimators=500; total time=   4.4s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=squared_error, n_estimators=100; total time=   0.4s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=squared_error, n_estimators=500; total time=   1.7s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=absolute_error, n_estimators=100; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=absolute_error, n_estimators=100; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=absolute_error, n_estimators=200; total time=   1.9s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=absolute_error, n_estimators=500; total time=   3.1s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=absolute_error, n_estimators=500; total time=   3.0s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=huber, n_estimators=200; total time=   1.7s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=huber, n_estimators=500; total time=   4.2s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=squared_error, n_estimators=100; total time=   0.3s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=squared_error, n_estimators=500; total time=   1.7s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=absolute_error, n_estimators=100; total time=   0.6s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=absolute_error, n_estimators=100; total time=   0.6s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=absolute_error, n_estimators=200; total time=   1.2s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=absolute_error, n_estimators=200; total time=   1.2s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=absolute_error, n_estimators=500; total time=   2.8s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=huber, n_estimators=100; total time=   0.8s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=huber, n_estimators=200; total time=   1.6s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=huber, n_estimators=500; total time=   4.2s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=huber, n_estimators=500; total time=   4.7s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=absolute_error, n_estimators=100; total time=   0.6s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=absolute_error, n_estimators=100; total time=   0.6s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=absolute_error, n_estimators=200; total time=   1.2s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=absolute_error, n_estimators=200; total time=   1.2s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=absolute_error, n_estimators=500; total time=   3.0s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=huber, n_estimators=100; total time=   0.8s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=huber, n_estimators=200; total time=   1.6s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=huber, n_estimators=200; total time=   1.7s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=huber, n_estimators=500; total time=   4.1s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=squared_error, n_estimators=500; total time=   1.6s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=squared_error, n_estimators=500; total time=   1.6s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=absolute_error, n_estimators=100; total time=   0.6s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=absolute_error, n_estimators=200; total time=   1.2s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=absolute_error, n_estimators=500; total time=   3.0s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=huber, n_estimators=100; total time=   0.8s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=huber, n_estimators=100; total time=   0.8s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=huber, n_estimators=200; total time=   1.6s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=huber, n_estimators=500; total time=   4.8s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=squared_error, n_estimators=100; total time=   0.3s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=squared_error, n_estimators=100; total time=   0.3s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=squared_error, n_estimators=100; total time=   0.3s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=squared_error, n_estimators=500; total time=   1.7s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=squared_error, n_estimators=500; total time=   1.6s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=absolute_error, n_estimators=200; total time=   1.2s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=absolute_error, n_estimators=200; total time=   1.2s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=absolute_error, n_estimators=500; total time=   3.1s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=huber, n_estimators=100; total time=   0.8s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=huber, n_estimators=200; total time=   1.7s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=huber, n_estimators=500; total time=   3.9s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=huber, n_estimators=500; total time=   3.9s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=squared_error, n_estimators=500; total time=   1.7s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=absolute_error, n_estimators=200; total time=   1.3s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=absolute_error, n_estimators=500; total time=   2.9s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=huber, n_estimators=100; total time=   0.8s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=huber, n_estimators=100; total time=   0.8s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=huber, n_estimators=200; total time=   1.6s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=huber, n_estimators=200; total time=   2.0s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=huber, n_estimators=500; total time=   4.1s\n[CV] END criterion=squared_error, learning_rate=0.2, loss=squared_error, n_estimators=200; total time=   0.6s\n[CV] END criterion=squared_error, learning_rate=0.2, loss=squared_error, n_estimators=200; total time=   0.6s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=squared_error, n_estimators=100; total time=   0.4s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=squared_error, n_estimators=500; total time=   1.8s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=absolute_error, n_estimators=100; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=absolute_error, n_estimators=100; total time=   0.8s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=absolute_error, n_estimators=200; total time=   1.4s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=absolute_error, n_estimators=200; total time=   1.4s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=absolute_error, n_estimators=500; total time=   3.2s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=huber, n_estimators=100; total time=   0.9s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=huber, n_estimators=200; total time=   1.8s\n[CV] END criterion=friedman_mse, learning_rate=0.1, loss=huber, n_estimators=500; total time=   4.4s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=squared_error, n_estimators=100; total time=   0.4s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=squared_error, n_estimators=100; total time=   0.4s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=squared_error, n_estimators=100; total time=   0.4s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=squared_error, n_estimators=100; total time=   0.4s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=squared_error, n_estimators=500; total time=   1.7s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=squared_error, n_estimators=500; total time=   1.8s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=absolute_error, n_estimators=100; total time=   1.0s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=absolute_error, n_estimators=200; total time=   1.2s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=absolute_error, n_estimators=500; total time=   2.9s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=huber, n_estimators=100; total time=   0.8s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=huber, n_estimators=100; total time=   0.8s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=huber, n_estimators=200; total time=   1.7s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=huber, n_estimators=500; total time=   4.3s\n[CV] END criterion=friedman_mse, learning_rate=0.2, loss=huber, n_estimators=500; total time=   4.1s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=squared_error, n_estimators=500; total time=   1.7s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=absolute_error, n_estimators=200; total time=   1.2s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=absolute_error, n_estimators=500; total time=   2.8s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=huber, n_estimators=100; total time=   0.8s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=huber, n_estimators=100; total time=   0.8s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=huber, n_estimators=200; total time=   1.6s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=huber, n_estimators=200; total time=   1.6s\n[CV] END criterion=friedman_mse, learning_rate=0.4, loss=huber, n_estimators=500; total time=   4.2s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=squared_error, n_estimators=200; total time=   0.8s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=squared_error, n_estimators=500; total time=   2.2s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=absolute_error, n_estimators=100; total time=   0.6s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=absolute_error, n_estimators=100; total time=   0.7s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=absolute_error, n_estimators=200; total time=   1.3s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=absolute_error, n_estimators=500; total time=   3.0s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=huber, n_estimators=100; total time=   0.8s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=huber, n_estimators=100; total time=   0.8s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=huber, n_estimators=200; total time=   1.6s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=huber, n_estimators=500; total time=   4.1s\n[CV] END criterion=friedman_mse, learning_rate=0.6, loss=huber, n_estimators=500; total time=   4.0s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=squared_error, n_estimators=500; total time=   1.6s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=absolute_error, n_estimators=200; total time=   1.1s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=absolute_error, n_estimators=200; total time=   1.2s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=absolute_error, n_estimators=500; total time=   3.1s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=huber, n_estimators=100; total time=   0.8s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=huber, n_estimators=200; total time=   1.6s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=huber, n_estimators=500; total time=   4.4s\n[CV] END criterion=friedman_mse, learning_rate=0.8, loss=huber, n_estimators=500; total time=   4.2s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=squared_error, n_estimators=500; total time=   1.6s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=absolute_error, n_estimators=200; total time=   1.2s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=absolute_error, n_estimators=500; total time=   2.8s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=huber, n_estimators=100; total time=   0.8s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=huber, n_estimators=100; total time=   0.8s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=huber, n_estimators=200; total time=   1.7s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=huber, n_estimators=200; total time=   1.6s\n[CV] END criterion=friedman_mse, learning_rate=0.9, loss=huber, n_estimators=500; total time=   4.0s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=squared_error, n_estimators=200; total time=   0.6s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=squared_error, n_estimators=200; total time=   0.7s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=squared_error, n_estimators=500; total time=   1.6s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=absolute_error, n_estimators=100; total time=   0.6s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=absolute_error, n_estimators=100; total time=   0.7s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=absolute_error, n_estimators=200; total time=   1.2s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=absolute_error, n_estimators=200; total time=   1.2s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=absolute_error, n_estimators=500; total time=   2.9s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=huber, n_estimators=100; total time=   0.8s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=huber, n_estimators=200; total time=   1.6s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=huber, n_estimators=500; total time=   4.2s\n[CV] END criterion=squared_error, learning_rate=0.1, loss=huber, n_estimators=500; total time=   3.9s\n[CV] END criterion=squared_error, learning_rate=0.2, loss=squared_error, n_estimators=500; total time=   1.6s\n[CV] END criterion=squared_error, learning_rate=0.2, loss=absolute_error, n_estimators=100; total time=   0.6s\n[CV] END criterion=squared_error, learning_rate=0.2, loss=absolute_error, n_estimators=200; total time=   1.2s\n","output_type":"stream"},{"execution_count":188,"output_type":"execute_result","data":{"text/plain":"GridSearchCV(cv=5, estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid=[{'criterion': ['friedman_mse', 'squared_error'],\n                          'learning_rate': [0.1, 0.2, 0.4, 0.6, 0.8, 0.9],\n                          'loss': ['squared_error', 'absolute_error', 'huber'],\n                          'n_estimators': [100, 200, 500]}],\n             verbose=2)","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid=[{&#x27;criterion&#x27;: [&#x27;friedman_mse&#x27;, &#x27;squared_error&#x27;],\n                          &#x27;learning_rate&#x27;: [0.1, 0.2, 0.4, 0.6, 0.8, 0.9],\n                          &#x27;loss&#x27;: [&#x27;squared_error&#x27;, &#x27;absolute_error&#x27;, &#x27;huber&#x27;],\n                          &#x27;n_estimators&#x27;: [100, 200, 500]}],\n             verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=GradientBoostingRegressor(), n_jobs=-1,\n             param_grid=[{&#x27;criterion&#x27;: [&#x27;friedman_mse&#x27;, &#x27;squared_error&#x27;],\n                          &#x27;learning_rate&#x27;: [0.1, 0.2, 0.4, 0.6, 0.8, 0.9],\n                          &#x27;loss&#x27;: [&#x27;squared_error&#x27;, &#x27;absolute_error&#x27;, &#x27;huber&#x27;],\n                          &#x27;n_estimators&#x27;: [100, 200, 500]}],\n             verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","source":"gbr_grid.best_params_","metadata":{"execution":{"iopub.status.busy":"2024-01-10T05:07:25.516690Z","iopub.execute_input":"2024-01-10T05:07:25.517418Z","iopub.status.idle":"2024-01-10T05:07:25.524361Z","shell.execute_reply.started":"2024-01-10T05:07:25.517381Z","shell.execute_reply":"2024-01-10T05:07:25.523196Z"},"trusted":true},"execution_count":190,"outputs":[{"execution_count":190,"output_type":"execute_result","data":{"text/plain":"{'criterion': 'friedman_mse',\n 'learning_rate': 0.2,\n 'loss': 'absolute_error',\n 'n_estimators': 500}"},"metadata":{}}]},{"cell_type":"code","source":"gbr_grid.best_score_","metadata":{"execution":{"iopub.status.busy":"2024-01-10T05:07:32.082371Z","iopub.execute_input":"2024-01-10T05:07:32.082798Z","iopub.status.idle":"2024-01-10T05:07:32.089454Z","shell.execute_reply.started":"2024-01-10T05:07:32.082761Z","shell.execute_reply":"2024-01-10T05:07:32.088372Z"},"trusted":true},"execution_count":191,"outputs":[{"execution_count":191,"output_type":"execute_result","data":{"text/plain":"0.9122838602478558"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\ngbr=GradientBoostingRegressor(criterion= 'friedman_mse',learning_rate= 0.2,loss='absolute_error',n_estimators= 500).fit(x_train,y_train)\nypred5=gbr.predict(x_test)\nr2_score(y_test,ypred5)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T05:13:11.144750Z","iopub.execute_input":"2024-01-10T05:13:11.145923Z","iopub.status.idle":"2024-01-10T05:13:13.247675Z","shell.execute_reply.started":"2024-01-10T05:13:11.145883Z","shell.execute_reply":"2024-01-10T05:13:13.246601Z"},"trusted":true},"execution_count":199,"outputs":[{"execution_count":199,"output_type":"execute_result","data":{"text/plain":"0.9261833134105502"},"metadata":{}}]},{"cell_type":"markdown","source":" Taking parameters :{'criterion': 'friedman_mse',\n 'learning_rate': 0.2,\n 'loss': 'absolute_error',\n 'n_estimators': 500} as found by gridsearchcv. ","metadata":{}}]}